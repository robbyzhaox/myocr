{"config":{"lang":["en","zh"],"separator":"[\\s\\u200b\\-_,:!=\\[\\: )\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MyOCR Documentation","text":"<p>MyOCR is a Python library designed to streamline the development and deployment of production-ready OCR systems.</p> <p>Whether you need basic text extraction or complex structured data extraction from documents, MyOCR provides the tools to build robust and efficient pipelines.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>\ud83d\ude80 End-to-End &amp; Modular: Build complete OCR workflows (Detection, Classification, Recognition) by combining modular components.</li> <li>\ud83d\udee0\ufe0f Extensible: Easily integrate custom models or processing logic.</li> <li>\u26a1 Production Ready: Optimized for speed with ONNX Runtime support for CPU and GPU inference.</li> <li>\ud83d\udcca Structured Output: Extract information into predefined JSON formats using LLM integration.</li> <li>\ud83d\udd0c Multiple Usage Modes: Use as a Python library, deploy as a REST API service, or run in Docker.</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<ol> <li>Installation: Set up MyOCR and download necessary models.</li> <li>Overview: Understand the core concepts (Models, Predictors, Pipelines).</li> <li>Inference Guide: Learn how to run OCR tasks using the library.</li> </ol>"},{"location":"#core-concepts-deep-dive","title":"Core Concepts Deep Dive","text":"<ul> <li>Models: Learn about the supported model types (ONNX, PyTorch, Custom) and architectures.</li> <li>Predictors: Understand how models are wrapped with pre/post-processing for specific tasks.</li> <li>Pipelines: Explore the high-level pipelines that orchestrate predictors for end-to-end OCR.</li> </ul>"},{"location":"#deployment-options","title":"Deployment Options","text":"<p>Beyond using MyOCR as a Python library (see Inference Guide), you can also deploy it:</p> <ul> <li>As a REST API:<ul> <li>Start the built-in Flask server: <code>python main.py</code> (runs on port 5000 by default).</li> <li>Endpoints: <code>GET /ping</code>, <code>POST /ocr</code> (basic OCR), <code>POST /ocr-json</code> (structured OCR).</li> <li>A separate UI is available: doc-insight-ui</li> </ul> </li> <li>Using Docker:<ul> <li>Build CPU/GPU images using <code>Dockerfile-infer-CPU</code> or <code>Dockerfile-infer-GPU</code>.</li> <li>Use the helper script: <code>scripts/build_docker_image.sh</code> for easy setup.</li> <li>Example run: <code>docker run -d -p 8000:8000 myocr:gpu</code> (exposes service on port 8000).</li> </ul> </li> </ul>"},{"location":"#additional-resources","title":"Additional Resources","text":"<ul> <li>FAQ: Find answers to common questions.</li> <li>Changelog: See recent updates and changes.</li> <li>Contributing Guidelines: Learn how to contribute to the project.</li> <li>GitHub Repository: Source code, issues, and discussions.</li> </ul>"},{"location":"#license","title":"License","text":"<p>MyOCR is open-sourced under the Apache 2.0 License.</p>"},{"location":"CHANGELOG/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"CHANGELOG/#unreleased","title":"Unreleased","text":""},{"location":"CONTRIBUTING/","title":"Contributing to MyOCR","text":"<p>Thank you for your interest in contributing to MyOCR! This document provides guidelines and instructions for contributing to this project.</p>"},{"location":"CONTRIBUTING/#code-of-conduct","title":"Code of Conduct","text":"<p>By participating in this project, you agree to maintain a respectful and inclusive environment for everyone. Please be kind, considerate, and constructive in your communication.</p>"},{"location":"CONTRIBUTING/#getting-started","title":"Getting Started","text":"<ol> <li>Fork the repository: Create your own fork of the repository on GitHub.</li> <li>Clone your fork:     <pre><code>git clone https://github.com/your-username/myocr.git\ncd myocr\n</code></pre></li> <li>Add the upstream repository:    <pre><code>git remote add upstream https://github.com/robbyzhaox/myocr.git\n</code></pre></li> <li>Create a branch: Create a new branch for your work.    <pre><code>git checkout -b feature/your-feature-name\n</code></pre></li> </ol>"},{"location":"CONTRIBUTING/#development-setup","title":"Development Setup","text":"<ol> <li> <p>Install dependencies:    <pre><code>pip install -e \".[dev]\"\n</code></pre>    This installs the package in development mode with all development dependencies.</p> </li> <li> <p>Set up pre-commit hooks (optional but recommended):    <pre><code>pre-commit install\n</code></pre></p> </li> </ol>"},{"location":"CONTRIBUTING/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Keep changes focused: Each PR should address a specific feature, bug fix, or improvement.</li> <li>Update documentation: Ensure that documentation is updated to reflect your changes.</li> <li>Write tests: Add or update tests for the changes you've made.</li> <li>Run tests locally: Make sure all tests pass before submitting your PR.    <pre><code>pytest\n</code></pre></li> <li>Submit the PR: Push your changes to your fork and create a PR against the main repository.    <pre><code>git push origin feature/your-feature-name\n</code></pre></li> <li>PR Description: Provide a clear description of the changes and reference any related issues.</li> <li>Code Review: Be responsive to code review comments and make necessary adjustments.</li> </ol>"},{"location":"CONTRIBUTING/#coding-standards","title":"Coding Standards","text":"<p>We use several tools to enforce coding standards. The easiest way to ensure your code meets these standards is by using the provided Makefile commands:</p>"},{"location":"CONTRIBUTING/#using-the-makefile","title":"Using the Makefile","text":"<pre><code># Format all code (isort, black, ruff fix)\nmake run-format\n\n# Run code quality checks (isort, black, ruff, mypy, pytest)\nmake run-checks\n</code></pre>"},{"location":"CONTRIBUTING/#individual-tools","title":"Individual Tools","text":"<p>If you prefer to run the tools individually:</p> <ol> <li> <p>Black: For code formatting    <pre><code>black .\n</code></pre></p> </li> <li> <p>isort: For import sorting    <pre><code>isort .\n</code></pre></p> </li> <li> <p>Ruff: For linting    <pre><code>ruff check .\n</code></pre></p> </li> <li> <p>mypy: For type checking    <pre><code>mypy myocr\n</code></pre></p> </li> </ol> <p>The configuration for these tools is in the <code>pyproject.toml</code> file.</p>"},{"location":"CONTRIBUTING/#testing-guidelines","title":"Testing Guidelines","text":"<ol> <li>Write unit tests: Write comprehensive tests for new features and bug fixes.</li> <li>Test Coverage: Aim for high test coverage for all new code.</li> <li>Test Directory Structure: </li> <li>Place tests in the <code>tests/</code> directory</li> <li>Follow the same directory structure as the source code</li> </ol>"},{"location":"CONTRIBUTING/#documentation","title":"Documentation","text":"<p>Good documentation is crucial for the project:</p> <ol> <li>Docstrings: Add docstrings to all public classes and functions.</li> <li>Example Usage: Include example usage in docstrings where appropriate.</li> <li>README Updates: Update the README if you add major features or change functionality.</li> <li>API Documentation: For significant additions, consider updating the API documentation.</li> </ol>"},{"location":"CONTRIBUTING/#building-documentation","title":"Building Documentation","text":"<p>You can build the documentation locally using:</p> <pre><code>make docs\n</code></pre> <p>This command will generate HTML documentation and start a local server to view it.</p>"},{"location":"CONTRIBUTING/#issue-reporting","title":"Issue Reporting","text":"<p>Before creating a new issue:</p> <ol> <li>Check existing issues: Make sure the issue hasn't already been reported.</li> <li>Provide information: Include detailed information about the problem:</li> <li>Steps to reproduce</li> <li>Expected behavior</li> <li>Actual behavior</li> <li>Environment (OS, Python version, etc.)</li> <li>Logs or error messages</li> <li>Use templates: If available, use the issue templates provided in the repository.</li> </ol>"},{"location":"CONTRIBUTING/#adding-new-features","title":"Adding New Features","text":"<p>When proposing new features:</p> <ol> <li>Discuss first: For major features, open an issue to discuss the feature before implementing it.</li> <li>Modular approach: Keep the modular architecture in mind when designing new features.</li> <li>Pipeline integration: Ensure that new components integrate well with the existing pipeline structure.</li> <li>Model compatibility: If adding new models, ensure they can be loaded with the existing ModelZoo system.</li> </ol>"},{"location":"CONTRIBUTING/#docker-development","title":"Docker Development","text":"<p>We provide a utility script to simplify the Docker build and deployment process:</p>"},{"location":"CONTRIBUTING/#using-the-build-script","title":"Using the Build Script","text":"<p>The <code>scripts/build_docker_image.sh</code> script automates the process of building and running a Docker container:</p> <pre><code># Make the script executable if it's not already\nchmod +x scripts/build_docker_image.sh\n\n# Run the script\n./scripts/build_docker_image.sh\n</code></pre> <p>This script: 1. Stops and removes any existing containers based on the MyOCR image 2. Removes any existing MyOCR Docker images 3. Copies models from your local configuration 4. Builds a new Docker image using the GPU-enabled Dockerfile 5. Runs a container exposing the service on port 8000</p>"},{"location":"CONTRIBUTING/#manual-docker-build","title":"Manual Docker Build","text":"<p>If you prefer to build the Docker image manually, or need to customize the process:</p> <pre><code># For GPU version\ndocker build -f Dockerfile-infer-GPU -t myocr:custom .\n\n# For CPU version\ndocker build -f Dockerfile-infer-CPU -t myocr:custom-cpu .\n\n# Run with custom options\ndocker run -d -p 8000:8000 -v /path/to/local/models:/app/models myocr:custom\n</code></pre>"},{"location":"CONTRIBUTING/#license","title":"License","text":"<p>By contributing to MyOCR, you agree that your contributions will be licensed under the project's Apache 2.0 license.</p> <p>Thank you for contributing to MyOCR! Your efforts help make this project better for everyone. </p>"},{"location":"faq/","title":"FAQ - Frequently Asked Questions","text":""},{"location":"faq/#q-im-having-trouble-downloading-the-models-using-the-curl-commands","title":"Q: I'm having trouble downloading the models using the <code>curl</code> commands.","text":"<p>A: The download links point to Google Drive.  1.  Ensure <code>curl</code> is installed and can access external URLs, especially handling redirects (<code>-L</code> flag should be used, which is included in the example). 2.  You might encounter permission issues or download quotas with Google Drive. Try accessing the link directly in your browser to download the <code>.onnx</code> files (<code>dbnet++.onnx</code>, <code>rec.onnx</code>, <code>cls.onnx</code>). 3.  Once downloaded, manually place these files into the default model directory: <code>~/.MyOCR/models/</code> (create the directory if it doesn't exist). Adjust the path for Windows if necessary (e.g., <code>~/AppData/Local/MyOCR/models/</code>).</p>"},{"location":"faq/#q-where-does-myocr-look-for-models-by-default","title":"Q: Where does MyOCR look for models by default?","text":"<p>A: The default path is configured in <code>myocr/config.py</code> (<code>MODEL_PATH</code>) and usually resolves to <code>~/.MyOCR/models/</code> on Linux/macOS. Pipeline configuration files (<code>myocr/pipelines/config/*.yaml</code>) reference model filenames relative to this directory. You can change <code>MODEL_PATH</code> or use absolute paths in the YAML configuration if you store models elsewhere.</p>"},{"location":"faq/#q-how-do-i-switch-between-cpu-and-gpu-inference","title":"Q: How do I switch between CPU and GPU inference?","text":"<p>A: When initializing pipelines or models, pass a <code>Device</code> object from <code>myocr.modeling.model</code>.  *   For GPU (assuming CUDA is set up): <code>Device('cuda:0')</code> (for the first GPU). *   For CPU: <code>Device('cpu')</code>. Ensure you have the correct <code>onnxruntime</code> package installed (<code>onnxruntime</code> for CPU, <code>onnxruntime-gpu</code> for GPU) and compatible CUDA drivers for GPU usage.</p>"},{"location":"faq/#q-the-structuredoutputocrpipeline-isnt-working-or-gives-errors","title":"Q: The <code>StructuredOutputOCRPipeline</code> isn't working or gives errors.","text":"<p>A: This pipeline relies on an external Large Language Model (LLM). 1.  Check Configuration: Ensure the <code>myocr/pipelines/config/structured_output_pipeline.yaml</code> file has the correct <code>model</code>, <code>base_url</code>, and <code>api_key</code> for your chosen LLM provider (e.g., OpenAI, Ollama, a local server). 2.  API Key: Make sure the API key is correctly specified (either directly in the YAML or via an environment variable if the YAML points to one, like <code>OPENAI_API_KEY</code>). 3.  Connectivity: Verify that your environment can reach the <code>base_url</code> specified for the LLM API. 4.  Schema: Ensure the Pydantic <code>json_schema</code> passed during initialization is valid and the descriptions guide the LLM effectively.</p>"},{"location":"faq/#q-whats-the-difference-between-a-predictor-and-a-pipeline","title":"Q: What's the difference between a Predictor and a Pipeline?","text":"<p>A: *   Predictor: A lower-level component that wraps a single <code>Model</code> with its specific pre-processing (<code>convert_input</code>) and post-processing (<code>convert_output</code>) logic (defined in a <code>ParamConverter</code>). It handles one specific task (e.g., text detection). *   Pipeline: A higher-level component that orchestrates multiple <code>Predictors</code> to perform a complete workflow (e.g., end-to-end OCR combining detection, classification, and recognition). Pipelines provide the main user-facing interface for common tasks.</p>"},{"location":"faq/#q-how-can-i-use-my-own-custom-models","title":"Q: How can I use my own custom models?","text":"<p>A: *   ONNX Models: Place your <code>.onnx</code> file in the model directory and update the relevant pipeline configuration YAML file (<code>myocr/pipelines/config/*.yaml</code>) to point to your model's filename. See the Overview section. *   Custom PyTorch Models: Define your model architecture using components from <code>myocr/modeling/</code> (backbones, necks, heads) and create a YAML configuration file specifying the architecture. Load it using <code>ModelLoader().load(model_format='custom', ...)</code> or create a custom pipeline/predictor. See the Models Documentation for details on <code>CustomModel</code> and YAML configuration.</p>"},{"location":"faq/#q-i-see-errors-related-to-pyclipper-or-shapely-during-detection","title":"Q: I see errors related to <code>pyclipper</code> or <code>shapely</code> during detection.","text":"<p>A: These are dependencies used for geometric operations (like expanding bounding boxes) in the <code>TextDetectionParamConverter</code>. Ensure they were installed correctly along with MyOCR. You might need to install their binary dependencies separately depending on your OS (e.g., <code>libgeos-dev</code> on Debian/Ubuntu for Shapely). Check the installation logs or try reinstalling <code>pip install -e .</code>.</p>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide covers the necessary steps to install MyOCR and its dependencies.</p>"},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python: Version 3.11 or higher is required.</li> <li>CUDA: Version 12.6 or higher is recommended for GPU acceleration. CPU-only mode is also supported.</li> <li>Operating System: Linux, macOS, or Windows.</li> </ul>"},{"location":"getting-started/installation/#installation-steps","title":"Installation Steps","text":"<ol> <li> <p>Clone the Repository:</p> <pre><code>git clone https://github.com/robbyzhaox/myocr.git\ncd myocr\n</code></pre> </li> <li> <p>Install Dependencies:</p> <ul> <li>For standard usage: <pre><code># Installs the package and required dependencies\npip install -e .\n</code></pre></li> <li>For development (including testing, linting, etc.): <pre><code># Installs standard dependencies plus development tools\npip install -e \".[dev]\"\n</code></pre></li> </ul> </li> <li> <p>Download Pre-trained Models:</p> <p>MyOCR relies on pre-trained models for its default pipelines. These need to be downloaded manually.</p> <pre><code># Create the default model directory if it doesn't exist\n# On Linux/macOS:\nmkdir -p ~/.MyOCR/models/\n# On Windows (using Git Bash or similar):\n# mkdir -p ~/AppData/Local/MyOCR/models/\n# Note: Adjust the Windows path if needed based on your environment.\n\n# Download models (ensure curl is installed)\n# Detection Model (DBNet++)\ncurl -L \"https://drive.google.com/uc?export=download&amp;id=1b5I8Do4ODU9xE_dinDGZMraq4GDgHPH9\" -o ~/.MyOCR/models/dbnet++.onnx\n# Recognition Model (CRNN-like)\ncurl -L \"https://drive.google.com/uc?export=download&amp;id=1MSF7ArwmRjM4anDiMnqhlzj1GE_J7gnX\" -o ~/.MyOCR/models/rec.onnx\n# Classification Model (Angle)\ncurl -L \"https://drive.google.com/uc?export=download&amp;id=1TCu3vAXNVmPBY2KtoEBTGOE6tpma0puX\" -o ~/.MyOCR/models/cls.onnx\n</code></pre> <ul> <li>Note: The default location where MyOCR expects models is <code>~/.MyOCR/models/</code>. This path is defined in <code>myocr/config.py</code>. You can modify this configuration or place models elsewhere if needed, but you would need to adjust the paths in the pipeline configuration files (<code>myocr/pipelines/config/*.yaml</code>).</li> <li>The <code>curl</code> commands above use Google Drive links. Ensure you can download from these links in your environment. You might need to adjust the commands or download the files manually if <code>curl</code> has issues with redirects or permissions.</li> </ul> </li> </ol>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Once installation is complete and models are downloaded, you can proceed to:</p> <ul> <li>Overview: Get a high-level understanding of the library.</li> <li>Inference Guide: See examples of how to run OCR tasks.</li> </ul>"},{"location":"getting-started/overview/","title":"Overview","text":"<p>Welcome to MyOCR! This library provides a powerful and flexible framework for building and deploying Optical Character Recognition (OCR) pipelines.</p>"},{"location":"getting-started/overview/#why-myocr","title":"Why MyOCR?","text":"<p>MyOCR is designed with production readiness and developer experience in mind. Key features include:</p> <ul> <li>End-to-End Workflow: Seamlessly integrates text detection, direction classification, and text recognition.</li> <li>Modular &amp; Extensible: Easily swap models, pre/post-processing steps (via Converters), or entire pipelines.</li> <li>Optimized for Production: Leverages ONNX Runtime for high-performance CPU and GPU inference.</li> <li>Structured Data Extraction: Go beyond raw text with pipelines that extract information into structured formats (like JSON) using LLMs.</li> <li>Developer-Friendly: Offers clean Python APIs and pre-built components to get started quickly.</li> </ul>"},{"location":"getting-started/overview/#core-components","title":"Core Components","text":"<p>MyOCR is built around several key concepts:</p>"},{"location":"getting-started/overview/#components-diagram","title":"Components Diagram","text":""},{"location":"getting-started/overview/#class-diagram","title":"Class Diagram","text":"<ul> <li>Model: Represents a neural network model. MyOCR supports loading ONNX models (<code>OrtModel</code>), standard PyTorch models (<code>PyTorchModel</code>), and custom PyTorch models defined by YAML configurations (<code>CustomModel</code>). Models handle the core computation.<ul> <li>See the Models Section for more details.</li> </ul> </li> <li>Converter (<code>ParamConverter</code>): Prepares input data for a model and processes the model's raw output into a more usable format. Each predictor uses a specific converter.<ul> <li>See the Predictors Section for converter specifics.</li> </ul> </li> <li>Predictor: Combines a <code>Model</code> and a <code>ParamConverter</code> to perform a specific inference task (e.g., text detection). It provides a user-friendly interface, accepting standard inputs (like PIL Images) and returning processed results (like bounding boxes).<ul> <li>See the Predictors Section for available predictors.</li> </ul> </li> <li>Pipeline: Orchestrates multiple <code>Predictors</code> to perform complex, multi-step tasks like end-to-end OCR. Pipelines offer the highest-level interface for most common use cases.<ul> <li>See the Pipelines Section for available pipelines.</li> </ul> </li> </ul>"},{"location":"getting-started/overview/#customization-and-extension","title":"Customization and Extension","text":"<p>MyOCR's modular design allows for easy customization.</p>"},{"location":"getting-started/overview/#adding-new-structured-output-schemas","title":"Adding New Structured Output Schemas","text":"<p>The <code>StructuredOutputOCRPipeline</code> uses Pydantic models to define the desired output format. You can easily create your own:</p> <ol> <li> <p>Define your data model using Pydantic:</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import List, Optional # Optional import if needed\n\nclass CustomDataSchema(BaseModel):\n    customer_name: Optional[str] = Field(None, description=\"The name of the customer\")\n    order_id: str = Field(..., description=\"The unique order identifier\") # Use ... for required fields\n    # Add more fields with descriptions...\n</code></pre> </li> <li> <p>Pass your model when creating the pipeline:</p> <pre><code>from myocr.pipelines import StructuredOutputOCRPipeline\nfrom myocr.modeling.model import Device\n# from your_module import CustomDataSchema # Import your defined model\n\n# Assuming CustomDataSchema is defined as above\npipeline = StructuredOutputOCRPipeline(device=Device(\"cuda:0\"), json_schema=CustomDataSchema)\n</code></pre> </li> </ol>"},{"location":"getting-started/overview/#replacing-or-adding-new-models-onnx","title":"Replacing or Adding New Models (ONNX)","text":"<p>If you have your own ONNX models for detection, classification, or recognition:</p> <ol> <li> <p>Place Model Files: Copy your <code>.onnx</code> model files to the default model directory (<code>~/.MyOCR/models/</code>) or another location.</p> </li> <li> <p>Update Configuration: Modify the relevant pipeline configuration YAML file (e.g., <code>myocr/pipelines/config/common_ocr_pipeline.yaml</code>) to point to your new model files. Use paths relative to the main model directory specified in <code>myocr.config.MODEL_PATH</code> (which defaults to <code>~/.MyOCR/models/</code>).</p> <pre><code># Example modification in myocr/pipelines/config/common_ocr_pipeline.yaml\nmodel:\n  detection: \"your_custom_detection_model.onnx\" # Assumes file is in ~/.MyOCR/models/\n  cls_direction: \"your_custom_cls_model.onnx\"\n  recognition: \"path/relative/to/model_dir/your_rec_model.onnx\" # Example if in a subdirectory\n</code></pre> <p>Refer to the specific pipeline documentation for details on its configuration file.</p> </li> </ol>"},{"location":"inference/local/","title":"Inference","text":"<p>This section describes how to perform inference using the MyOCR library, primarily by utilizing the pre-defined pipelines.</p>"},{"location":"inference/local/#using-pipelines-for-inference","title":"Using Pipelines for Inference","text":"<p>The recommended way to perform end-to-end OCR is through the pipeline classes provided in <code>myocr.pipelines</code>. These pipelines handle the loading of necessary models and the orchestration of detection, classification (optional), and recognition steps.</p>"},{"location":"inference/local/#standard-ocr-with-commonocrpipeline","title":"Standard OCR with <code>CommonOCRPipeline</code>","text":"<p>This pipeline is suitable for general OCR tasks where the goal is to extract all text and its location from an image.</p> <pre><code>import logging\nfrom myocr.pipelines import CommonOCRPipeline\nfrom myocr.modeling.model import Device\nfrom PIL import Image\n\n# Configure logging (optional)\nlogging.basicConfig(level=logging.INFO)\n\n# --- 1. Initialization ---\n# Specify the device ('cuda:0' for GPU 0, 'cpu' for CPU)\ndevice = Device('cuda:0')\n\n# Initialize the pipeline\n# This will load the default models specified in \n# myocr/pipelines/config/common_ocr_pipeline.yaml\ntry:\n    ocr_pipeline = CommonOCRPipeline(device=device)\n    logging.info(\"CommonOCRPipeline initialized successfully.\")\nexcept Exception as e:\n    logging.error(f\"Error initializing pipeline: {e}\")\n    exit()\n\n# --- 2. Processing ---\nimage_path = 'path/to/your/document.jpg' # Or .png, .tif, etc.\n\ntry:\n    logging.info(f\"Processing image: {image_path}\")\n    # The process method handles image loading, detection, classification, and recognition\n    results = ocr_pipeline.process(image_path)\nexcept FileNotFoundError:\n    logging.error(f\"Image file not found: {image_path}\")\n    exit()\nexcept Exception as e:\n    logging.error(f\"Error during OCR processing: {e}\")\n    exit()\n\n# --- 3. Handling Results ---\nif results:\n    logging.info(\"OCR processing completed.\")\n    # Get combined text content\n    full_text = results.get_content_text()\n    print(\"--- Full Recognized Text ---\")\n    print(full_text)\n    print(\"-----------------------------\")\n\n    # Access individual text boxes and their properties\n    print(\"--- Individual Boxes ---\")\n    for text_item in results.text_items:\n        box = text_item.bounding_box\n        print(f\"Text: \\\"{text_item.text}\\\"\")\n        print(f\"  Confidence: {text_item.confidence:.4f}\")\n        # BoundingBox details (left, top, right, bottom, angle, score from detection)\n        print(f\"  Box Coords (L,T,R,B): ({box.left}, {box.top}, {box.right}, {box.bottom})\") \n        print(f\"  Angle: {box.angle[0]} degrees, Confidence: {box.angle[1]:.4f}\") # Angle is tuple (pred_angle, confidence)\n        print(f\"  Detection Score: {box.score:.4f}\")\n        print(\"---\")\nelse:\n    logging.warning(f\"No text detected in image: {image_path}\")\n</code></pre>"},{"location":"inference/local/#structured-data-extraction-with-structuredoutputocrpipeline","title":"Structured Data Extraction with <code>StructuredOutputOCRPipeline</code>","text":"<p>This pipeline is used when you need to extract specific information from a document and format it as JSON, based on a predefined schema.</p> <pre><code>import logging\nfrom myocr.pipelines import StructuredOutputOCRPipeline\nfrom myocr.modeling.model import Device\nfrom pydantic import BaseModel, Field\nimport os\n\n# Configure logging (optional)\nlogging.basicConfig(level=logging.INFO)\n\n# --- 1. Define Output Schema ---\n# Use Pydantic to define the structure of the information you want to extract.\nclass ReceiptInfo(BaseModel):\n    store_name: Optional[str] = Field(None, description=\"Name of the store or vendor\")\n    total_amount: Optional[float] = Field(None, description=\"The final total amount paid\")\n    transaction_date: Optional[str] = Field(None, description=\"Date of the transaction (YYYY-MM-DD)\")\n    items: List[str] = Field([], description=\"List of purchased items mentioned\")\n\n# --- 2. Initialization ---\ndevice = Device('cuda:0')\n\n# Ensure your OpenAI API Key is set (or provide directly)\n# Assumes the key is in an environment variable OPENAI_API_KEY\n# The pipeline config yaml (structured_output_pipeline.yaml) should point to this key\nif \"OPENAI_API_KEY\" not in os.environ:\n  logging.warning(\"OPENAI_API_KEY environment variable not set. LLM extraction might fail.\")\n  # You might need to manually set the api_key in the config yaml or modify the pipeline code\n\ntry:\n    # Initialize pipeline with the device and the desired output schema\n    structured_pipeline = StructuredOutputOCRPipeline(device=device, json_schema=ReceiptInfo)\n    logging.info(\"StructuredOutputOCRPipeline initialized successfully.\")\nexcept Exception as e:\n    logging.error(f\"Error initializing structured pipeline: {e}\")\n    exit()\n\n# --- 3. Processing ---\nimage_path = 'path/to/your/receipt.png'\n\ntry:\n    logging.info(f\"Processing image for structured extraction: {image_path}\")\n    # This process involves OCR followed by LLM-based extraction\n    extracted_data = structured_pipeline.process(image_path)\nexcept FileNotFoundError:\n    logging.error(f\"Image file not found: {image_path}\")\n    exit()\nexcept Exception as e:\n    # This could be an OCR error or an error during LLM interaction (e.g., API key issue, network error)\n    logging.error(f\"Error during structured OCR processing: {e}\")\n    exit()\n\n# --- 4. Handling Results ---\nif extracted_data:\n    logging.info(\"Structured data extraction completed.\")\n    # The result is a Pydantic model instance\n    print(\"--- Extracted Information (JSON) ---\")\n    print(extracted_data.model_dump_json(indent=2))\n    print(\"-------------------------------------\")\n\n    # You can access fields directly\n    # print(f\"Store: {extracted_data.store_name}\")\n    # print(f\"Total: {extracted_data.total_amount}\")\nelse:\n    # This might happen if OCR found no text or the LLM failed to extract data matching the schema\n    logging.warning(f\"Could not extract structured data from image: {image_path}\")\n</code></pre>"},{"location":"inference/local/#direct-predictor-usage-advanced","title":"Direct Predictor Usage (Advanced)","text":"<p>While pipelines are recommended, you can use individual predictors directly if you need more granular control over the process (e.g., using only detection, or providing pre-processed inputs). Refer to the Predictors section documentation for details on initializing and using each predictor/converter pair.</p>"},{"location":"inference/local/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Device Selection: Using a CUDA-enabled GPU (<code>Device('cuda:0')</code>) significantly speeds up inference compared to CPU (<code>Device('cpu')</code>). Ensure you have the necessary drivers and ONNX Runtime GPU build installed.</li> <li>Model Choice: The specific ONNX models configured in the pipeline YAML files impact performance and accuracy.</li> <li>Batch Processing: While the current pipeline examples process single images, predictors often handle batch inputs internally (e.g., processing all detected boxes simultaneously in recognition). For processing many images, consider parallel execution or batching at the application level if needed. </li> </ul>"},{"location":"inference/rest/","title":"Inference via REST API","text":"<p>MyOCR provides a built-in RESTful API service based on Flask, allowing you to perform OCR tasks via HTTP requests. This is useful for integrating MyOCR into web applications, microservices, or accessing it from different programming languages.</p> <p>You can run this API service directly or deploy it using Docker.</p>"},{"location":"inference/rest/#option-1-running-the-flask-api-directly","title":"Option 1: Running the Flask API Directly","text":"<p>This method runs the API service directly on your host machine.</p> <p>1. Prerequisites:</p> <ul> <li>Ensure you have completed the Installation steps, including installing dependencies and downloading models.</li> <li>Make sure you are in the root directory of the <code>myocr</code> project.</li> </ul> <p>2. Start the Server:</p> <pre><code># Start the Flask development server\n# It will typically run on http://127.0.0.1:5000 by default\npython main.py \n</code></pre> <ul> <li>The server uses the models and configurations defined within the project.</li> <li>By default, it might use the device (CPU/GPU) configured in the underlying pipeline settings or attempt auto-detection. Check the server logs for details.</li> </ul> <p>3. API Endpoints:</p> <ul> <li><code>GET /ping</code>: Checks if the service is running. Returns a simple confirmation.     <pre><code>curl http://127.0.0.1:5000/ping\n</code></pre></li> <li><code>POST /ocr</code>: Performs basic OCR on an uploaded image.<ul> <li>Request: Send a <code>POST</code> request with the image file included as <code>multipart/form-data</code>. The file part should be named <code>file</code>. <pre><code>curl -X POST -F \"file=@/path/to/your/image.jpg\" http://127.0.0.1:5000/ocr \n</code></pre></li> <li>Response: Returns a JSON object containing the recognized text and bounding box information (similar to the output of <code>CommonOCRPipeline</code>).</li> </ul> </li> <li><code>POST /ocr-json</code>: Performs OCR and extracts structured information based on a schema.<ul> <li>Request: Send a <code>POST</code> request with the image file (<code>file</code>) and the desired JSON schema (<code>schema_json</code>) as <code>multipart/form-data</code>.<ul> <li><code>schema_json</code>: A JSON string representing the Pydantic model schema (including descriptions for fields). <pre><code># Example using the pre-defined InvoiceModel schema (get schema first if needed)\n# NOTE: Generating the correct schema_json might require a helper script or knowing the exact format expected by the API.\n# This example assumes schema_json contains the JSON representation of InvoiceModel.schema()\nSCHEMA='{...}' # Replace with actual JSON schema string\n\ncurl -X POST \\\n  -F \"file=@/path/to/your/invoice.png\" \\\n  -F \"schema_json=$SCHEMA\" \\\n  http://127.0.0.1:5000/ocr-json\n</code></pre></li> </ul> </li> <li>Response: Returns a JSON object matching the provided schema, populated with the extracted data.</li> </ul> </li> </ul> <p>4. Optional UI:</p> <p>A separate Streamlit-based UI is available for interacting with these endpoints: doc-insight-ui.</p>"},{"location":"inference/rest/#option-2-deploying-with-docker","title":"Option 2: Deploying with Docker","text":"<p>Docker provides a containerized environment for running the API service, ensuring consistency across different machines.</p> <p>1. Prerequisites:</p> <ul> <li>Docker installed.</li> <li>For GPU support: NVIDIA Container Toolkit installed.</li> <li>Ensure models are downloaded to the default location (<code>~/.MyOCR/models/</code>) on the host machine before building the image, as the Docker build process might copy them.</li> </ul> <p>2. Build the Docker Image:</p> <p>Choose the appropriate Dockerfile:</p> <ul> <li>For GPU Inference: <pre><code>docker build -f Dockerfile-infer-GPU -t myocr:gpu .\n</code></pre></li> <li>For CPU Inference: <pre><code>docker build -f Dockerfile-infer-CPU -t myocr:cpu .\n</code></pre></li> </ul> <p>3. Run the Docker Container:</p> <ul> <li>GPU Version: Expose the container's port (usually 8000 or 5000 internally, check the Dockerfile) to a host port (e.g., 8000). Requires the <code>--gpus all</code> flag.     <pre><code># Map host port 8000 to container port 8000\ndocker run -d --gpus all -p 8000:8000 --name myocr-service myocr:gpu\n</code></pre></li> <li>CPU Version: <pre><code># Map host port 8000 to container port 8000\ndocker run -d -p 8000:8000 --name myocr-service myocr:cpu\n</code></pre></li> <li>Note: The internal port the application listens on inside the container might vary (check <code>EXPOSE</code> in the Dockerfile or the <code>CMD</code> instruction). The <code>-p</code> flag maps <code>HOST_PORT:CONTAINER_PORT</code>.</li> </ul> <p>4. Using the Helper Script (Easier Setup):</p> <p>The project includes a script to simplify building and running the GPU version:</p> <p><pre><code># Make executable (if needed)\nchmod +x scripts/build_docker_image.sh\n\n# Run the script (stops old containers, cleans images, builds, runs)\n./scripts/build_docker_image.sh\n</code></pre> This script typically maps port 8000 on the host to the container's service port.</p> <p>5. Accessing API Endpoints (Docker):</p> <p>Once the container is running, access the API endpoints using the host machine's IP/hostname and the mapped host port (e.g., 8000 in the examples above):</p> <pre><code># Example Ping (assuming port 8000 is mapped)\ncurl http://localhost:8000/ping \n\n# Example Basic OCR (assuming port 8000 is mapped)\ncurl -X POST -F \"file=@/path/to/your/image.jpg\" http://localhost:8000/ocr\n</code></pre> <p>Remember to replace <code>/path/to/your/image.jpg</code> with the actual path on the machine where you are running the <code>curl</code> command.</p>"},{"location":"models/model-list/","title":"Models","text":"<p>This section provides details about the deep learning models used within the MyOCR project for tasks like text detection, recognition, and direction classification.</p>"},{"location":"models/model-list/#model-loading-and-management","title":"Model Loading and Management","text":"<p>MyOCR utilizes a flexible model loading system defined in <code>myocr/modeling/model.py</code>. It supports loading models in different formats:</p> <ul> <li>ONNX (<code>OrtModel</code>): Loads and runs optimized models using the ONNX Runtime (<code>onnxruntime</code>). This is often preferred for inference due to performance benefits.</li> <li>PyTorch (<code>PyTorchModel</code>): Loads standard PyTorch models, potentially leveraging pre-defined architectures from libraries like <code>torchvision</code>.</li> <li>Custom PyTorch (<code>CustomModel</code>): Loads custom PyTorch models defined via YAML configuration files. These configurations specify the model's architecture, including backbones, necks, and heads, using components defined within <code>myocr/modeling/</code>.</li> </ul> <p>A <code>ModelLoader</code> class acts as a factory to instantiate the correct model type based on the specified format (<code>onnx</code>, <code>pt</code>, <code>custom</code>).</p> <pre><code># Example (Conceptual)\nfrom myocr.modeling.model import ModelLoader, Device\n\n# Load an ONNX model for CPU inference\nloader = ModelLoader()\nonnx_model = loader.load(model_format='onnx', model_name_path='path/to/your/model.onnx', device=Device('cpu'))\n\n# Load a custom model defined by YAML for GPU inference\ncustom_model = loader.load(model_format='custom', model_name_path='path/to/your/config.yaml', device=Device('cuda:0'))\n</code></pre>"},{"location":"models/model-list/#model-architectures","title":"Model Architectures","text":"<p>The <code>myocr/modeling/</code> directory houses the building blocks for custom models:</p> <ul> <li><code>architectures/</code>: Defines the overall structure connecting backbones, necks, and heads. (e.g., <code>DBNet</code>, <code>CRNN</code>).</li> <li><code>backbones/</code>: Contains feature extraction networks (e.g., <code>ResNet</code>, <code>MobileNetV3</code>).</li> <li><code>necks/</code>: Includes feature fusion modules (e.g., <code>FPN</code> - Feature Pyramid Network).</li> <li><code>heads/</code>: Defines the final layers responsible for specific tasks (e.g., detection probability maps, sequence decoding).</li> </ul>"},{"location":"models/model-list/#common-models-used","title":"Common Models Used","text":"<p>While the system is flexible, common models used in MyOCR include:</p> <ul> <li>Text Detection: Often based on DBNet or DBNet++, utilizing architectures like ResNet combined with FPN necks and specialized detection heads. These models output segmentation maps indicating text presence and location.</li> <li>Text Recognition: Frequently employs CRNN (Convolutional Recurrent Neural Network) architectures. These typically use CNN backbones (like ResNet or VGG) followed by BiLSTM layers and a CTC (Connectionist Temporal Classification) head for sequence decoding.</li> <li>Text Direction Classification: Usually simpler CNN-based classifiers (e.g., adapted MobileNet or ResNet variants) trained to predict text orientation (e.g., 0 or 180 degrees).</li> </ul>"},{"location":"models/model-list/#configuration","title":"Configuration","text":"<p>Custom models are configured using YAML files. These files specify:</p> <ul> <li>The overall <code>Architecture</code> (linking backbones, necks, heads).</li> <li>Specific parameters for each component (e.g., backbone type, feature dimensions).</li> <li>Optionally, a path to <code>pretrained</code> weights.</li> </ul> <pre><code># Example config.yaml (Simplified)\nArchitecture:\n  model_type: det # or rec, cls\n  backbone:\n    name: MobileNetV3\n    scale: 0.5\n    pretrained: true\n  neck:\n    name: DBFPN\n    out_channels: 256\n  head:\n    name: DBHead\n    k: 50\n\npretrained: path/to/pretrained_weights.pth # Optional\n</code></pre>"},{"location":"models/model-list/#model-conversion","title":"Model Conversion","text":"<p>The <code>CustomModel</code> class provides a <code>to_onnx</code> method to export PyTorch models defined by YAML configurations into the ONNX format for optimized inference.</p> <pre><code># Example (Conceptual)\n# Assuming 'custom_model' is an instance of CustomModel\n# and 'dummy_input' is a sample input tensor matching the model's expected input shape/type\ncustom_model.to_onnx('exported_model.onnx', dummy_input)\n</code></pre> <p>This allows leveraging the performance benefits of ONNX Runtime even for models developed using the custom PyTorch framework within MyOCR.</p>"},{"location":"models/model-list/#available-models","title":"Available Models","text":""},{"location":"models/model-list/#text-detection-models","title":"Text Detection Models","text":"<ul> <li>DBNet++: A state-of-the-art text detection model based on DBNet architecture</li> <li>Input: RGB image</li> <li>Output: Text region polygons</li> <li>Features:<ul> <li>High accuracy for arbitrary-shaped text</li> <li>Fast inference speed</li> <li>Robust to various text orientations</li> </ul> </li> </ul>"},{"location":"models/model-list/#text-recognition-models","title":"Text Recognition Models","text":"<ul> <li>CRNN: A hybrid CNN-RNN model for text recognition</li> <li>Input: Cropped text region</li> <li>Output: Recognized text</li> <li>Features:<ul> <li>Supports Chinese and English characters</li> <li>Handles variable-length text</li> <li>Robust to different fonts and styles</li> </ul> </li> </ul>"},{"location":"models/model-list/#text-classification-models","title":"Text Classification Models","text":"<ul> <li>Text Direction Classifier: Determines text orientation</li> <li>Input: Text region</li> <li>Output: Orientation angle</li> <li>Features:<ul> <li>0\u00b0 and 180\u00b0 classification</li> <li>Helps improve recognition accuracy</li> </ul> </li> </ul>"},{"location":"models/model-list/#model-architecture","title":"Model Architecture","text":""},{"location":"models/model-list/#text-detection-dbnet","title":"Text Detection (DBNet++)","text":"<pre><code># Architecture overview\nBackbone: ResNet\nNeck: FPN\nHead: DBHead\n</code></pre> <p>Key components: - Feature Pyramid Network (FPN) for multi-scale feature extraction - Differentiable Binarization (DB) for accurate text region detection - Post-processing for polygon generation</p>"},{"location":"models/model-list/#text-recognition-crnn","title":"Text Recognition (CRNN)","text":"<pre><code># Architecture overview\nBackbone: CNN\nNeck: BiLSTM\nHead: CTC\n</code></pre> <p>Key components: - Convolutional layers for feature extraction - Bidirectional LSTM for sequence modeling - Connectionist Temporal Classification (CTC) for text decoding</p>"},{"location":"models/model-list/#model-configuration","title":"Model Configuration","text":"<p>Models can be configured through YAML files:</p> <pre><code>model:\n  detection:\n    input_size: [640, 640]\n    mean: [0.485, 0.456, 0.406]\n    std: [0.229, 0.224, 0.225]\n\n  recognition:\n    input_size: [100, 32]\n    mean: [0.5, 0.5, 0.5]\n    std: [0.5, 0.5, 0.5]\n</code></pre>"},{"location":"models/model-list/#model-performance","title":"Model Performance","text":""},{"location":"models/model-list/#accuracy","title":"Accuracy","text":"<ul> <li>Text Detection:</li> <li>ICDAR2015: 85.2% F1-score</li> <li> <p>Total-Text: 82.1% F1-score</p> </li> <li> <p>Text Recognition:</p> </li> <li>Chinese: 92.3% accuracy</li> <li>English: 94.7% accuracy</li> </ul>"},{"location":"models/model-list/#speed","title":"Speed","text":"<p>On NVIDIA T4 GPU: - Detection: ~50ms per image - Recognition: ~20ms per text region - Classification: ~10ms per region</p>"},{"location":"models/model-list/#model-training","title":"Model Training","text":"<p>See the Training Guide for instructions on training custom models. </p>"},{"location":"models/new-model/","title":"Adding New Models","text":"<p>MyOCR's modular design allows you to integrate new or custom models into the system. The process depends on the type of model you are adding.</p>"},{"location":"models/new-model/#option-1-adding-a-pre-trained-onnx-model","title":"Option 1: Adding a Pre-trained ONNX Model","text":"<p>This is the simplest way, especially if your model fits one of the standard tasks (detection, classification, recognition) and has compatible input/output formats with existing <code>ParamConverter</code> classes.</p> <ol> <li> <p>Place the Model File:</p> <ul> <li>Copy your pre-trained <code>.onnx</code> model file into the default model directory (<code>~/.MyOCR/models/</code>) or another location accessible by your application.</li> </ul> </li> <li> <p>Update Pipeline Configuration:</p> <ul> <li>Identify the pipeline that will use your model (e.g., <code>CommonOCRPipeline</code>).</li> <li>Edit its corresponding YAML configuration file (e.g., <code>myocr/pipelines/config/common_ocr_pipeline.yaml</code>).</li> <li>Modify the <code>model:</code> section to point to your new model's filename. If the model is in the default directory, just the filename is needed. If it's elsewhere, you might need to adjust <code>myocr.config.MODEL_PATH</code> or use absolute paths (less recommended).</li> </ul> <pre><code># Example in myocr/pipelines/config/common_ocr_pipeline.yaml\nmodel:\n  detection: \"your_new_detection_model.onnx\" # Replace default with yours\n  cls_direction: \"cls.onnx\" # Keep default or replace\n  recognition: \"your_new_recognition_model.onnx\" # Replace default with yours\n</code></pre> </li> <li> <p>Verify Compatibility:</p> <ul> <li>Ensure your ONNX model's input and output shapes/types are compatible with the <code>ParamConverter</code> used by the pipeline for that step (e.g., <code>TextDetectionParamConverter</code> for detection). If not, you might need to create a custom converter (see Option 3).</li> </ul> </li> </ol>"},{"location":"models/new-model/#option-2-adding-a-custom-pytorch-model-architecture-weights","title":"Option 2: Adding a Custom PyTorch Model (Architecture &amp; Weights)","text":"<p>If you have a custom model defined in PyTorch (using components potentially from <code>myocr.modeling</code> or external libraries), you can integrate it using MyOCR's custom model loading.</p> <ol> <li> <p>Define Model Architecture (if new):</p> <ul> <li>If your architecture isn't already defined, you might need to implement its components (e.g., new backbones, heads) following the structure within <code>myocr/modeling/</code>.</li> </ul> </li> <li> <p>Create YAML Configuration:</p> <ul> <li>Create a <code>.yaml</code> file that defines how your architecture components are connected. This file specifies the classes for the backbone, neck (optional), and head, along with their parameters.</li> <li>Optionally, include a <code>pretrained:</code> key pointing to a <code>.pth</code> file containing the trained weights for the entire model.</li> </ul> <pre><code># Example: config/my_custom_detector.yaml\nArchitecture:\n  model_type: det\n  backbone:\n    name: YourCustomBackbone # Class name under myocr.modeling.backbones\n    param1: value1\n  neck:\n    name: YourCustomNeck\n    param2: value2\n  head:\n    name: YourCustomHead\n    param3: value3\n\npretrained: /path/to/your/custom_model_weights.pth # Optional: Full model weights\n</code></pre> </li> <li> <p>Load the Custom Model:</p> <ul> <li>Use the <code>ModelLoader</code> or <code>CustomModel</code> class to load your model using its YAML configuration.</li> </ul> <pre><code>from myocr.modeling.model import ModelLoader, Device\n\nloader = ModelLoader()\ndevice = Device('cuda:0')\ncustom_model = loader.load(\n    model_format='custom',\n    model_name_path='config/my_custom_detector.yaml',\n    device=device\n)\n</code></pre> </li> <li> <p>Create Predictor (with appropriate Converter):</p> <ul> <li>You will likely need a <code>ParamConverter</code> that matches your custom model's input pre-processing and output post-processing needs. You might be able to reuse an existing one (e.g., <code>TextDetectionParamConverter</code> if your output is similar) or you may need to create a custom converter class inheriting from <code>myocr.base.ParamConverter</code>.</li> </ul> <pre><code># Option A: Reuse existing converter (if compatible)\nfrom myocr.predictors import TextDetectionParamConverter\npredictor = custom_model.predictor(TextDetectionParamConverter(custom_model.device))\n\n# Option B: Create and use a custom converter\n# from my_custom_converters import MyCustomParamConverter \n# predictor = custom_model.predictor(MyCustomParamConverter(...))\n</code></pre> </li> <li> <p>Integrate into a Pipeline (Optional):</p> <ul> <li>You can use your custom predictor directly or integrate it into a custom pipeline class that inherits from <code>myocr.base.Pipeline</code>.</li> </ul> </li> </ol>"},{"location":"models/new-model/#option-3-creating-a-custom-paramconverter","title":"Option 3: Creating a Custom <code>ParamConverter</code>","text":"<p>If your model (ONNX or PyTorch) has unique input requirements or produces output in a format not handled by existing converters, you'll need to create a custom <code>ParamConverter</code>.</p> <ol> <li> <p>Inherit from <code>ParamConverter</code>:</p> <ul> <li>Create a new Python class that inherits from <code>myocr.base.ParamConverter</code>.</li> </ul> </li> <li> <p>Implement <code>convert_input</code>:</p> <ul> <li>This method takes the user-provided input (e.g., a PIL Image, <code>DetectedObjects</code>) and transforms it into the exact format (e.g., <code>numpy</code> array with specific shape, dtype, normalization) expected by your model's <code>forward</code> or <code>run</code> method.</li> </ul> </li> <li> <p>Implement <code>convert_output</code>:</p> <ul> <li>This method takes the raw output from your model (e.g., <code>numpy</code> arrays, tensors) and transforms it into a structured, user-friendly format (e.g., <code>DetectedObjects</code>, <code>RecognizedTexts</code>, or a custom Pydantic model).</li> </ul> </li> <li> <p>Use with Predictor:</p> <ul> <li>When creating a predictor from your model, pass an instance of your custom converter.</li> </ul> </li> </ol> <pre><code>from myocr.base import ParamConverter\nfrom myocr.predictors.base import DetectedObjects # Or other input/output types\nimport numpy as np\n\nclass MyCustomConverter(ParamConverter[np.ndarray, DetectedObjects]): # Example: Input numpy, Output DetectedObjects\n    def __init__(self, model_device):\n        super().__init__()\n        self.device = model_device\n        # Add any other needed params (thresholds, labels, etc.)\n\n    def convert_input(self, input_data: np.ndarray) -&gt; Optional[np.ndarray]:\n        # --- Add your custom preprocessing --- \n        # Example: normalize, resize, transpose, add batch dim\n        processed_input = ... \n        return processed_input\n\n    def convert_output(self, internal_result: np.ndarray) -&gt; Optional[DetectedObjects]:\n        # --- Add your custom postprocessing --- \n        # Example: decode bounding boxes, apply NMS, format results\n        formatted_output = ... \n        return formatted_output\n\n# Usage:\n# loaded_model = ... # Load your model (ONNX or Custom PyTorch)\n# custom_predictor = loaded_model.predictor(MyCustomConverter(loaded_model.device))\n</code></pre> <p>Refer to the implementations of existing converters in <code>myocr/predictors/</code> (e.g., <code>text_detection_predictor.py</code>) for concrete examples.</p>"},{"location":"models/train-model/","title":"Training Custom Models","text":"<p>MyOCR allows training custom models defined using its PyTorch-based modeling components (<code>myocr.modeling</code>). The core idea is to leverage the <code>CustomModel</code> class loaded from a YAML configuration within a standard PyTorch training loop.</p> <p>Disclaimer: This guide outlines the general approach. The project includes a <code>myocr/training/</code> directory which might contain specific training scripts, utilities, loss functions, or dataset handlers tailored for MyOCR. It is highly recommended to explore the contents of <code>myocr/training/</code> for framework-specific implementations and helpers before writing a training loop from scratch.</p>"},{"location":"models/train-model/#1-prepare-your-data","title":"1. Prepare Your Data","text":"<ul> <li>Dataset: You'll need a labeled dataset suitable for your task (e.g., images with bounding boxes and transcriptions for OCR).</li> <li>PyTorch Dataset Class: Create a custom <code>torch.utils.data.Dataset</code> class to load your images and labels, and perform necessary initial transformations.</li> <li>DataLoader: Use <code>torch.utils.data.DataLoader</code> to create batches of data for training and validation.</li> </ul> <pre><code># Conceptual Example: Custom Dataset\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\n\nclass YourOCRDataset(Dataset):\n    def __init__(self, image_paths, labels, transform=None):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        label = self.labels[idx]\n        image = Image.open(img_path).convert('RGB')\n\n        if self.transform:\n            image = self.transform(image) # Apply augmentations/preprocessing\n\n        # Return image and label in the format expected by your model/loss\n        return image, label \n\n# --- Create Datasets and DataLoaders ---\ntrain_transform = ... # Define training transforms (augmentations, tensor conversion, normalization)\nval_transform = ...   # Define validation transforms (tensor conversion, normalization)\n\ntrain_dataset = YourOCRDataset(train_image_paths, train_labels, transform=train_transform)\nval_dataset = YourOCRDataset(val_image_paths, val_labels, transform=val_transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n</code></pre>"},{"location":"models/train-model/#2-configure-your-model-architecture-yaml","title":"2. Configure Your Model Architecture (YAML)","text":"<ul> <li>Define the architecture of the model you want to train in a YAML configuration file (e.g., <code>config/my_trainable_model.yaml</code>).</li> <li>Specify the backbone, neck (optional), and head components from <code>myocr.modeling</code> or your custom implementations.</li> <li>You might start training from scratch or load pre-trained weights for specific components (e.g., a pre-trained backbone specified within the <code>backbone</code> section of the YAML).</li> </ul> <pre><code># Example: config/my_trainable_model.yaml\nArchitecture:\n  model_type: rec # Or det, cls\n  backbone:\n    name: ResNet # Example\n    layers: 34\n    pretrained: true # Load ImageNet weights for backbone\n  neck:\n    name: FPN # Example\n    out_channels: 256\n  head:\n    name: CTCHead # Example for recognition\n    num_classes: 9000 # Number of classes in your character set + blank\n\n# Optional: Load weights for the entire composed model (e.g., for fine-tuning)\n# pretrained: /path/to/your/full_model_weights.pth \n</code></pre>"},{"location":"models/train-model/#3-set-up-the-training-loop","title":"3. Set Up the Training Loop","text":"<ul> <li>Load Model: Use <code>ModelLoader</code> to load your <code>CustomModel</code> from the YAML configuration.</li> <li>Define Loss: Choose or implement a suitable loss function for your task (e.g., <code>torch.nn.CTCLoss</code> for recognition, custom loss for detection based on DBNet principles). Check <code>myocr/modeling/</code> or <code>myocr/training/</code> for potentially pre-defined losses.</li> <li>Define Optimizer: Select a PyTorch optimizer (e.g., <code>torch.optim.Adam</code>, <code>SGD</code>).</li> <li>Training Device: Set the device (CPU or GPU).</li> </ul> <pre><code>import torch\nimport torch.optim as optim\nfrom myocr.modeling.model import ModelLoader, Device\n\n# --- Configuration ---\nMODEL_CONFIG_PATH = 'config/my_trainable_model.yaml'\nLEARNING_RATE = 1e-4\nNUM_EPOCHS = 50\nOUTPUT_DIR = \"./trained_models\"\n\n# --- Setup ---\ndevice = Device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\n# Load the custom model structure\nloader = ModelLoader()\nmodel = loader.load(model_format='custom', model_name_path=MODEL_CONFIG_PATH, device=device)\n\n# Define Loss Function (Example for CTC)\n# criterion = torch.nn.CTCLoss(blank=0).to(device.name) \n# Or find/implement your specific loss\ncriterion = ... \n\n# Define Optimizer\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n# Optional: Learning rate scheduler\n# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n</code></pre>"},{"location":"models/train-model/#4-run-the-training-loop","title":"4. Run the Training Loop","text":"<ul> <li>Iterate through epochs and batches.</li> <li>Set model to training mode (<code>model.train()</code>).</li> <li>Perform forward pass, calculate loss, perform backpropagation, and update optimizer.</li> <li>Include a validation loop using <code>model.eval()</code> and <code>torch.no_grad()</code> to monitor performance.</li> <li>Save model checkpoints periodically (e.g., save the best performing model based on validation loss).</li> </ul> <pre><code>import os\n\nprint(f\"Starting training on {device.name}...\")\nbest_val_loss = float('inf')\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nfor epoch in range(NUM_EPOCHS):\n    # --- Training Phase ---\n    model.train() # Set model to training mode\n    running_loss = 0.0\n    for i, (inputs, labels) in enumerate(train_loader):\n        inputs = inputs.to(device.name)\n        # --- Format labels and inputs as required by your model and loss --- \n        # e.g., for CTC Loss, labels need target lengths\n        formatted_labels = ... \n        target_lengths = ... \n        input_lengths = ... # Often needed for CTC\n\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(inputs) # Model returns raw output for training\n\n        # Calculate Loss (adjust based on your criterion)\n        # loss = criterion(outputs.log_softmax(2), formatted_labels, input_lengths, target_lengths)\n        loss = ... # Calculate loss based on your specific setup\n\n        # Backward pass and optimize\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        if i % 100 == 99: # Print progress every 100 batches\n            print(f'[Epoch {epoch + 1}, Batch {i + 1}] Training loss: {running_loss / 100:.4f}')\n            running_loss = 0.0\n\n    # --- Validation Phase ---\n    model.eval() # Set model to evaluation mode\n    val_loss = 0.0\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            inputs = inputs.to(device.name)\n            # --- Format labels and inputs ---\n            formatted_labels = ...\n            target_lengths = ...\n            input_lengths = ...\n\n            outputs = model(inputs)\n            # loss = criterion(outputs.log_softmax(2), formatted_labels, input_lengths, target_lengths)\n            loss = ... # Calculate validation loss\n            val_loss += loss.item()\n\n    avg_val_loss = val_loss / len(val_loader)\n    print(f'Epoch {epoch + 1} - Validation Loss: {avg_val_loss:.4f}')\n\n    # --- Save Best Model --- \n    if avg_val_loss &lt; best_val_loss:\n        best_val_loss = avg_val_loss\n        best_model_path = os.path.join(OUTPUT_DIR, f\"best_model_epoch_{epoch+1}.pth\")\n        # Save only the model state_dict\n        torch.save(model.loaded_model.state_dict(), best_model_path) \n        print(f\"Saved new best model to {best_model_path}\")\n\n    # Optional: Update learning rate scheduler\n    # if scheduler: scheduler.step()\n\nprint('Finished Training')\n\n# Save the final model\nfinal_model_path = os.path.join(OUTPUT_DIR, \"final_model.pth\")\ntorch.save(model.loaded_model.state_dict(), final_model_path)\nprint(f\"Saved final model to {final_model_path}\")\n</code></pre>"},{"location":"models/train-model/#5-after-training","title":"5. After Training","text":"<ul> <li>Evaluation: Load your saved weights (<code>.pth</code> file) into the <code>CustomModel</code> (potentially by setting the <code>pretrained</code> key in the YAML config to your saved path) and run evaluation.</li> <li>ONNX Export (Optional): For optimized inference, you can convert your trained PyTorch model to ONNX format using the <code>to_onnx</code> method of the <code>CustomModel</code>.</li> </ul> <p><pre><code># Load the trained model (assuming YAML points to the saved .pth via 'pretrained' key)\n# trained_model = loader.load('custom', MODEL_CONFIG_PATH, device)\n\n# --- Or load state dict manually after loading architecture --- \nmodel_for_export = loader.load('custom', MODEL_CONFIG_PATH, device)\nmodel_for_export.loaded_model.load_state_dict(torch.load(best_model_path, map_location=device.name))\nmodel_for_export.eval()\n\n# Create a dummy input with the correct shape and type\ndummy_input = torch.randn(1, 3, 640, 640).to(device.name) # Adjust shape as needed\n\nonnx_output_path = os.path.join(OUTPUT_DIR, \"trained_model.onnx\")\n\nmodel_for_export.to_onnx(onnx_output_path, dummy_input)\nprint(f\"Exported model to {onnx_output_path}\")\n</code></pre> *   You can then use this exported ONNX model following the steps in Adding New Models.</p>"},{"location":"pipelines/build-pipeline/","title":"Building Custom Pipelines","text":"<p>MyOCR's pipelines orchestrate multiple predictors to perform complex tasks. While the library provides standard pipelines like <code>CommonOCRPipeline</code> and <code>StructuredOutputOCRPipeline</code>, you might need to create a custom pipeline for specific workflows, such as:</p> <ul> <li>Using different combinations of models or predictors.</li> <li>Adding custom pre-processing or post-processing steps.</li> <li>Integrating components beyond standard OCR (e.g., image enhancement, layout analysis before OCR).</li> <li>Handling different input/output types.</li> </ul> <p>This guide explains the steps to build your own pipeline.</p>"},{"location":"pipelines/build-pipeline/#1-inherit-from-basepipeline","title":"1. Inherit from <code>base.Pipeline</code>","text":"<p>All pipelines should inherit from the abstract base class <code>myocr.base.Pipeline</code>.</p> <pre><code>from myocr.base import Pipeline\n\nclass MyCustomPipeline(Pipeline):\n    def __init__(self, *args, **kwargs):\n        # Initialization logic here\n        pass\n\n    def process(self, input_data):\n        # Processing logic here\n        pass\n</code></pre>"},{"location":"pipelines/build-pipeline/#2-initialize-predictors-in-__init__","title":"2. Initialize Predictors in <code>__init__</code>","text":"<p>The <code>__init__</code> method is where you typically load the models and create the predictor instances your pipeline will use.</p> <ul> <li>Load Models: Use <code>myocr.modeling.model.ModelLoader</code> to load the necessary ONNX or custom PyTorch models.</li> <li>Instantiate Converters: Create instances of the required <code>ParamConverter</code> classes (e.g., <code>TextDetectionParamConverter</code>, <code>TextRecognitionParamConverter</code>, or custom ones).</li> <li>Create Predictors: Combine the loaded models and converters using the <code>model.predictor(converter)</code> method.</li> <li>Store Predictors: Store the created predictor instances as attributes of your pipeline class (e.g., <code>self.det_predictor</code>).</li> </ul> <pre><code>import logging\nfrom myocr.base import Pipeline\nfrom myocr.modeling.model import ModelLoader, Device\nfrom myocr.config import MODEL_PATH # Default model directory path\nfrom myocr.predictors import (\n    TextDetectionParamConverter,\n    TextDirectionParamConverter,\n    TextRecognitionParamConverter\n)\n# Import any custom converters or models if needed\n\nlogger = logging.getLogger(__name__)\n\nclass MyDetectionOnlyPipeline(Pipeline):\n    def __init__(self, device: Device, detection_model_name: str = \"dbnet++.onnx\"):\n        super().__init__()\n        self.device = device\n\n        try:\n            # --- Load Detection Model ---\n            det_model_path = MODEL_PATH + detection_model_name\n            det_model = ModelLoader().load(\"onnx\", det_model_path, self.device)\n\n            # --- Create Detection Predictor ---\n            # Uses the standard converter for detection models\n            det_converter = TextDetectionParamConverter(det_model.device)\n            self.det_predictor = det_model.predictor(det_converter)\n\n            logger.info(f\"DetectionOnlyPipeline initialized with {detection_model_name} on {device.name}\")\n\n        except Exception as e:\n            logger.error(f\"Failed to initialize MyDetectionOnlyPipeline: {e}\")\n            raise # Re-raise the exception to signal failure\n\n    def process(self, input_data):\n        # Implementation in the next step\n        pass\n</code></pre>"},{"location":"pipelines/build-pipeline/#3-implement-the-process-method","title":"3. Implement the <code>process</code> Method","text":"<p>This method defines the core logic of your pipeline. It takes the input data (e.g., an image path, a PIL Image), calls the <code>predict</code> method of the initialized predictors in sequence, handles intermediate results, and returns the final output.</p> <pre><code>from PIL import Image\nfrom typing import Optional\nfrom myocr.predictors.base import DetectedObjects # Import necessary data structures\n\nclass MyDetectionOnlyPipeline(Pipeline):\n    def __init__(self, device: Device, detection_model_name: str = \"dbnet++.onnx\"):\n        # ... (Initialization from previous step) ...\n        super().__init__()\n        self.device = device\n\n        try:\n            det_model_path = MODEL_PATH + detection_model_name\n            det_model = ModelLoader().load(\"onnx\", det_model_path, self.device)\n            det_converter = TextDetectionParamConverter(det_model.device)\n            self.det_predictor = det_model.predictor(det_converter)\n            logger.info(f\"DetectionOnlyPipeline initialized with {detection_model_name} on {device.name}\")\n        except Exception as e:\n            logger.error(f\"Failed to initialize MyDetectionOnlyPipeline: {e}\")\n            raise\n\n    def process(self, image_path: str) -&gt; Optional[DetectedObjects]:\n        \"\"\"Processes an image file and returns detected objects.\"\"\"\n        try:\n            # 1. Load Image (Example: handling path input)\n            logger.debug(f\"Loading image from: {image_path}\")\n            image = Image.open(image_path).convert(\"RGB\")\n        except FileNotFoundError:\n            logger.error(f\"Image file not found: {image_path}\")\n            return None\n        except Exception as e:\n            logger.error(f\"Error loading image {image_path}: {e}\")\n            return None\n\n        # 2. Run Detection Predictor\n        logger.debug(\"Running text detection predictor...\")\n        try:\n            detected_objects = self.det_predictor.predict(image)\n        except Exception as e:\n            logger.error(f\"Error during detection prediction: {e}\")\n            return None\n\n        # 3. Return Results\n        if detected_objects:\n            logger.info(f\"Detection successful: Found {len(detected_objects.bounding_boxes)} boxes.\")\n        else:\n            logger.info(\"Detection successful: No text boxes found.\")\n\n        return detected_objects # Return the output of the detection predictor\n</code></pre>"},{"location":"pipelines/build-pipeline/#example-combining-predictors-conceptual","title":"Example: Combining Predictors (Conceptual)","text":"<p>If you need multiple steps, you chain the predictor calls, passing the output of one step as the input to the next (if compatible).</p> <pre><code>class MyFullOCRPipeline(Pipeline):\n    def __init__(self, device: Device):\n        super().__init__()\n        self.device = device\n        # --- Load det, cls, rec models --- (Assume paths are correct)\n        det_model = ModelLoader().load(\"onnx\", MODEL_PATH + \"dbnet++.onnx\", device)\n        cls_model = ModelLoader().load(\"onnx\", MODEL_PATH + \"cls.onnx\", device)\n        rec_model = ModelLoader().load(\"onnx\", MODEL_PATH + \"rec.onnx\", device)\n\n        # --- Create predictors ---\n        self.det_predictor = det_model.predictor(TextDetectionParamConverter(device))\n        self.cls_predictor = cls_model.predictor(TextDirectionParamConverter())\n        self.rec_predictor = rec_model.predictor(TextRecognitionParamConverter())\n        logger.info(f\"MyFullOCRPipeline initialized on {device.name}\")\n\n    def process(self, image_path: str):\n        logger.debug(f\"Processing {image_path}\")\n        try:\n            image = Image.open(image_path).convert(\"RGB\")\n        except Exception as e:\n            logger.error(f\"Error loading image: {e}\")\n            return None\n\n        # Step 1: Detection\n        detected = self.det_predictor.predict(image)\n        if not detected or not detected.bounding_boxes:\n            logger.info(\"No text detected.\")\n            return None\n        logger.debug(f\"Detected {len(detected.bounding_boxes)} regions.\")\n\n        # Step 2: Classification\n        classified = self.cls_predictor.predict(detected)\n        if not classified:\n            logger.warning(\"Classification step failed, proceeding without angle correction.\")\n            classified = detected # Use original detections if classification fails\n        logger.debug(\"Classification complete.\")\n\n        # Step 3: Recognition\n        recognized_texts = self.rec_predictor.predict(classified)\n        if not recognized_texts:\n            logger.warning(\"Recognition step failed.\")\n            return None\n        logger.info(\"Recognition complete.\")\n\n        # Add original image size info if needed by consumers\n        recognized_texts.original(image.size[0], image.size[1])\n\n        return recognized_texts # Final result\n</code></pre>"},{"location":"pipelines/build-pipeline/#using-your-custom-pipeline","title":"Using Your Custom Pipeline","text":"<p>Once defined, you can import and use your custom pipeline just like the built-in ones.</p> <pre><code># from your_module import MyDetectionOnlyPipeline # Or MyFullOCRPipeline\nfrom myocr.modeling.model import Device\n\npipeline = MyDetectionOnlyPipeline(device=Device('cuda:0'))\nresults = pipeline.process('path/to/image.jpg')\n\nif results:\n    # Process the results from your custom pipeline\n    print(f\"Found {len(results.bounding_boxes)} text regions.\")\n</code></pre> <p>Remember to handle potential errors during model loading or prediction steps within your pipeline logic.</p>"},{"location":"pipelines/pipelines-list/","title":"Pipelines","text":"<p>MyOCR pipelines orchestrate multiple components (predictors, models) to perform end-to-end OCR tasks. They provide a high-level interface for processing images or documents.</p>"},{"location":"pipelines/pipelines-list/#available-pipelines","title":"Available Pipelines","text":""},{"location":"pipelines/pipelines-list/#1-commonocrpipeline","title":"1. <code>CommonOCRPipeline</code>","text":"<p>Defined in <code>myocr/pipelines/common_ocr_pipeline.py</code>.</p> <p>This pipeline performs standard OCR: text detection, optional text direction classification, and text recognition.</p> <p>Initialization:</p> <pre><code>from myocr.pipelines import CommonOCRPipeline\nfrom myocr.modeling.model import Device\n\n# Initialize pipeline for GPU (or 'cpu')\npipeline = CommonOCRPipeline(device=Device('cuda:0'))\n</code></pre> <p>Configuration:</p> <p>The pipeline loads configuration from <code>myocr/pipelines/config/common_ocr_pipeline.yaml</code>. This file specifies the paths to the ONNX models used for detection, classification, and recognition relative to the <code>MODEL_PATH</code> defined in <code>myocr.config</code>.</p> <pre><code># Example: myocr/pipelines/config/common_ocr_pipeline.yaml\nmodel:\n  detection: \"en_PP-OCRv3_det_infer.onnx\"\n  cls_direction: \"ch_ppocr_mobile_v2.0_cls_infer.onnx\"\n  recognition: \"en_PP-OCRv3_rec_infer.onnx\"\n</code></pre> <p>Processing:</p> <p>The <code>process</code> method takes the path to an image file.</p> <pre><code>image_path = 'path/to/your/image.png'\nocr_results = pipeline.process(image_path)\n\nif ocr_results:\n    # Access recognized text and bounding boxes\n    print(ocr_results.get_content_text())\n    for box_info in ocr_results.boxes:\n      print(f\"Box: {box_info.points}, Text: {box_info.text}\")\n</code></pre> <p>Workflow:</p> <ol> <li>Loads the image.</li> <li>Uses <code>TextDetectionPredictor</code> to find text regions.</li> <li>Uses <code>TextDirectionPredictor</code> to classify the orientation of detected regions.</li> <li>Uses <code>TextRecognitionPredictor</code> to recognize the text within each oriented region.</li> <li>Returns a result object containing bounding boxes, text, and potentially confidence scores (details depend on the <code>Predictor</code> implementation).</li> </ol>"},{"location":"pipelines/pipelines-list/#2-structuredoutputocrpipeline","title":"2. <code>StructuredOutputOCRPipeline</code>","text":"<p>Defined in <code>myocr/pipelines/structured_output_pipeline.py</code>.</p> <p>This pipeline extends <code>CommonOCRPipeline</code> by adding a step to extract structured information (e.g., JSON) from the recognized text using a large language model (LLM) via the <code>OpenAiChatExtractor</code>.</p> <p>Initialization:</p> <p>Requires a device and a Pydantic model defining the desired JSON output schema.</p> <pre><code>from myocr.pipelines import StructuredOutputOCRPipeline\nfrom myocr.modeling.model import Device\nfrom pydantic import BaseModel, Field\n\n# Define your desired output structure\nclass InvoiceData(BaseModel):\n    invoice_number: str = Field(description=\"The invoice number\")\n    total_amount: float = Field(description=\"The total amount due\")\n    due_date: str = Field(description=\"The payment due date\")\n\n# Initialize pipeline\npipeline = StructuredOutputOCRPipeline(device=Device('cuda:0'), json_schema=InvoiceData)\n</code></pre> <p>Configuration:</p> <p>This pipeline loads its specific configuration from <code>myocr/pipelines/config/structured_output_pipeline.yaml</code>, which includes settings for the <code>OpenAiChatExtractor</code> (LLM model name, API base URL, API key).</p> <pre><code># Example: myocr/pipelines/config/structured_output_pipeline.yaml\nchat_bot:\n  model: \"gpt-4o\"\n  base_url: \"https://api.openai.com/v1\"\n  api_key: \"YOUR_API_KEY\"\n</code></pre> <p>Processing:</p> <p>The <code>process</code> method takes an image path.</p> <pre><code>image_path = 'path/to/your/invoice.pdf'\nstructured_data = pipeline.process(image_path)\n\nif structured_data:\n    print(structured_data.model_dump_json(indent=2))\n    # Access extracted fields directly\n    # print(f\"Invoice Number: {structured_data.invoice_number}\")\n</code></pre> <p>Workflow:</p> <ol> <li>Performs standard OCR using the inherited <code>CommonOCRPipeline.process</code> method to get the raw recognized text.</li> <li>If text is found, it passes the text content to the <code>OpenAiChatExtractor</code>.</li> <li>The extractor interacts with the configured LLM, providing the text and the desired <code>json_schema</code> (Pydantic model) as instructions.</li> <li>The LLM attempts to extract the relevant information and format it according to the schema.</li> <li>Returns an instance of the provided Pydantic model populated with the extracted data.</li> </ol>"},{"location":"pipelines/pipelines-list/#customization","title":"Customization","text":"<p>Pipelines can be customized by:</p> <ul> <li>Modifying the <code>.yaml</code> configuration files to use different models.</li> <li>Creating new pipeline classes that inherit from <code>Pipeline</code> or existing pipelines.</li> <li>Integrating different predictors or extractors.</li> </ul>"},{"location":"pipelines/pipelines-list/#pipeline-configuration","title":"Pipeline Configuration","text":"<p>Pipelines can be configured through YAML files:</p> <pre><code>pipeline:\n  detection:\n    threshold: 0.3\n    unclip_ratio: 2.0\n\n  recognition:\n    batch_size: 32\n    max_length: 25\n\n  classification:\n    threshold: 0.5\n</code></pre>"},{"location":"pipelines/pipelines-list/#pipeline-components","title":"Pipeline Components","text":""},{"location":"pipelines/pipelines-list/#text-detection","title":"Text Detection","text":"<ul> <li>Input: RGB image</li> <li>Output: Text region polygons</li> <li>Post-processing:</li> <li>Non-maximum suppression</li> <li>Polygon generation</li> <li>Confidence filtering</li> </ul>"},{"location":"pipelines/pipelines-list/#text-recognition","title":"Text Recognition","text":"<ul> <li>Input: Cropped text regions</li> <li>Output: Recognized text</li> <li>Features:</li> <li>Character-level recognition</li> <li>Confidence scores</li> <li>Language support</li> </ul>"},{"location":"pipelines/pipelines-list/#text-classification","title":"Text Classification","text":"<ul> <li>Input: Text regions</li> <li>Output: Orientation angles</li> <li>Used for:</li> <li>Text direction correction</li> <li>Improving recognition accuracy</li> </ul>"},{"location":"pipelines/pipelines-list/#performance-optimization","title":"Performance Optimization","text":""},{"location":"pipelines/pipelines-list/#batch-processing","title":"Batch Processing","text":"<pre><code># Process multiple images\nresults = [pipeline(img_path) for img_path in image_paths]\n</code></pre>"},{"location":"pipelines/pipelines-list/#gpu-acceleration","title":"GPU Acceleration","text":"<pre><code># Use GPU for faster processing\npipeline = CommonOCRPipeline(\"cuda:0\")\n</code></pre>"},{"location":"pipelines/pipelines-list/#memory-management","title":"Memory Management","text":"<pre><code># Clear GPU memory\nimport torch\ntorch.cuda.empty_cache()\n</code></pre>"},{"location":"pipelines/pipelines-list/#error-handling","title":"Error Handling","text":"<p>Pipelines handle various error cases:</p> <ul> <li>Invalid image format</li> <li>Missing model files</li> <li>GPU out of memory</li> <li>Invalid configuration</li> </ul> <p>See the Troubleshooting Guide for common issues and solutions. </p>"},{"location":"predictors/new-predictor/","title":"Building Custom Predictors","text":"<p>Predictors in MyOCR act as the bridge between a loaded <code>Model</code> (ONNX or PyTorch) and the end-user or pipeline. They encapsulate the necessary pre-processing and post-processing logic required to make a model easily usable for a specific task.</p> <p>While MyOCR provides standard predictors (via converters like <code>TextDetectionParamConverter</code>, <code>TextRecognitionParamConverter</code>), you might need a custom predictor if:</p> <ul> <li>Your model requires unique input pre-processing (e.g., different normalization, resizing, input format).</li> <li>Your model produces output that needs custom decoding or formatting (e.g., different bounding box formats, specialized classification labels, structured output not handled by existing pipelines).</li> <li>You want to create a predictor for a completely new task beyond detection, recognition, or classification.</li> </ul> <p>The key to building a custom predictor is creating a custom <code>ParamConverter</code> class.</p>"},{"location":"predictors/new-predictor/#1-understand-the-role-of-paramconverter","title":"1. Understand the Role of <code>ParamConverter</code>","text":"<p>A predictor itself is a simple wrapper (defined in <code>myocr.base.Predictor</code>). The actual work happens within its associated <code>ParamConverter</code> (a class inheriting from <code>myocr.base.ParamConverter</code>). The converter has two main jobs:</p> <ol> <li><code>convert_input(user_input)</code>: Takes the data provided by the user or pipeline (e.g., a PIL Image) and transforms it into the precise format expected by the model's inference method (e.g., a normalized, batch-dimensioned NumPy array).</li> <li><code>convert_output(model_output)</code>: Takes the raw output from the model's inference method (e.g., NumPy arrays representing heatmaps or sequence probabilities) and transforms it into a user-friendly, structured format (e.g., a list of bounding boxes with text and scores, like <code>DetectedObjects</code> or <code>RecognizedTexts</code>).</li> </ol>"},{"location":"predictors/new-predictor/#2-create-a-custom-paramconverter-class","title":"2. Create a Custom <code>ParamConverter</code> Class","text":"<ol> <li>Inherit: Create a Python class that inherits from <code>myocr.base.ParamConverter</code>.</li> <li>Specify Types (Optional but Recommended): Use generics to indicate the expected input type for <code>convert_input</code> and the output type for <code>convert_output</code>. For example, <code>ParamConverter[PIL.Image.Image, DetectedObjects]</code> means it takes a PIL Image and returns <code>DetectedObjects</code>.</li> <li>Implement <code>__init__</code>: Initialize any necessary parameters, such as thresholds, label mappings, or references needed during conversion.</li> <li>Implement <code>convert_input</code>: Write the code to transform the input data into the model-ready format.</li> <li>Implement <code>convert_output</code>: Write the code to transform the raw model output into the desired structured result.</li> </ol> <pre><code>import logging\nfrom typing import Optional, Tuple, List, Any\nimport numpy as np\nfrom PIL import Image as PILImage\n\nfrom myocr.base import ParamConverter\n# Import any necessary base structures or create your own\nfrom myocr.predictors.base import BoundingBox \n\nlogger = logging.getLogger(__name__)\n\n# --- Define a custom output structure (Example) ---\nclass CustomResult:\n    def __init__(self, label: str, score: float, details: Any):\n        self.label = label\n        self.score = score\n        self.details = details\n\n    def __repr__(self):\n        return f\"CustomResult(label='{self.label}', score={self.score:.4f}, details={self.details})\"\n\n# --- Create the Custom Converter ---\n# Example: Takes a PIL Image, outputs a CustomResult\nclass MyTaskConverter(ParamConverter[PILImage.Image, CustomResult]):\n    def __init__(self, threshold: float = 0.5, target_size: Tuple[int, int] = (224, 224)):\n        super().__init__()\n        self.threshold = threshold\n        self.target_size = target_size\n        self.input_image_for_output = None # Store context if needed for output conversion\n        logger.info(f\"MyTaskConverter initialized with threshold={threshold}, target_size={target_size}\")\n\n    def convert_input(self, input_data: PILImage.Image) -&gt; Optional[np.ndarray]:\n        \"\"\"Prepares a PIL Image for a hypothetical classification model.\"\"\"\n        self.input_image_for_output = input_data # Save for later use if needed\n\n        try:\n            # 1. Resize\n            image_resized = input_data.resize(self.target_size, PILImage.Resampling.BILINEAR)\n\n            # 2. Convert to NumPy array\n            image_np = np.array(image_resized).astype(np.float32)\n\n            # 3. Normalize (Example: simple /255)\n            image_np /= 255.0\n\n            # 4. Add Batch Dimension and Channel Dimension if needed (e.g., HWC -&gt; NCHW)\n            if image_np.ndim == 2: # Grayscale\n                image_np = np.expand_dims(image_np, axis=-1)\n            # Assume model wants NCHW\n            image_np = np.expand_dims(image_np.transpose(2, 0, 1), axis=0) \n\n            logger.debug(f\"Converted input image to shape: {image_np.shape}\")\n            return image_np.astype(np.float32)\n\n        except Exception as e:\n            logger.error(f\"Error during input conversion: {e}\")\n            return None\n\n    def convert_output(self, internal_result: Any) -&gt; Optional[CustomResult]:\n        \"\"\"Processes the raw output of a hypothetical classification model.\"\"\"\n        try:\n            # Assume model output is a list/tuple containing a NumPy array of scores\n            scores = internal_result[0] # Example: [[0.1, 0.8, 0.1]]\n            if scores.ndim &gt; 1: # Handle potential batch dimension\n                scores = scores[0]\n\n            # 1. Find best prediction\n            pred_index = np.argmax(scores)\n            pred_score = float(scores[pred_index])\n\n            logger.debug(f\"Raw scores: {scores}, Predicted index: {pred_index}, Score: {pred_score}\")\n\n            # 2. Apply threshold\n            if pred_score &lt; self.threshold:\n                logger.info(f\"Prediction score {pred_score} below threshold {self.threshold}\")\n                return None # Or return a default/negative result\n\n            # 3. Map index to label (Assume a predefined mapping)\n            labels = [\"cat\", \"dog\", \"other\"] # Example labels\n            pred_label = labels[pred_index] if pred_index &lt; len(labels) else \"unknown\"\n\n            # 4. Format into CustomResult\n            # Include any extra details, potentially using self.input_image_for_output\n            result = CustomResult(label=pred_label, score=pred_score, details={\"original_size\": self.input_image_for_output.size})\n\n            return result\n\n        except Exception as e:\n            logger.error(f\"Error during output conversion: {e}\")\n            return None\n</code></pre>"},{"location":"predictors/new-predictor/#3-create-the-predictor-instance","title":"3. Create the Predictor Instance","text":"<p>Once you have your custom converter and have loaded your model, you can create the predictor instance.</p> <pre><code>from myocr.modeling.model import ModelLoader, Device\nfrom PIL import Image\n# Assume MyTaskConverter is defined as above\n\n# 1. Load your model (ONNX or Custom PyTorch)\nmodel_path = \"path/to/your/custom_model.onnx\" # Or path to YAML for CustomModel\nmodel_format = \"onnx\" # Or \"custom\"\ndevice = Device('cuda:0')\n\nloader = ModelLoader()\nmodel = loader.load(model_format, model_path, device)\n\n# 2. Instantiate your custom converter\ncustom_converter = MyTaskConverter(threshold=0.6, target_size=(256, 256)) # Use custom params if needed\n\n# 3. Create the predictor\ncustom_predictor = model.predictor(custom_converter)\n\n# 4. Use the predictor\ninput_image = Image.open(\"path/to/test_image.jpg\").convert(\"RGB\")\nprediction_result = custom_predictor.predict(input_image) # Returns CustomResult or None\n\nif prediction_result:\n    print(f\"Prediction: {prediction_result}\")\nelse:\n    print(\"Prediction failed or below threshold.\")\n</code></pre>"},{"location":"predictors/new-predictor/#4-integrate-into-a-pipeline-optional","title":"4. Integrate into a Pipeline (Optional)","text":"<p>If your custom predictor is part of a larger workflow, you can integrate it into a Custom Pipeline by initializing it within the pipeline's <code>__init__</code> method and calling its <code>predict</code> method within the pipeline's <code>process</code> method.</p> <p>By following these steps, you can create specialized predictors tailored to your specific models and tasks within the MyOCR framework. </p>"},{"location":"predictors/predictor-list/","title":"Predictors","text":"<p>Predictors are responsible for handling the inference logic for specific models (detection, recognition, classification) within MyOCR. They bridge the gap between raw model outputs and usable results by incorporating pre-processing and post-processing steps.</p> <p>Predictors are typically associated with a <code>Model</code> object and a <code>ParamConverter</code>.</p> <ul> <li>Model: Provides the core <code>forward_internal</code> method (e.g., ONNX session run, PyTorch model forward pass).</li> <li>ParamConverter: Handles the conversion of input data into the format expected by the model, and the conversion of the model's raw output into a structured, meaningful format.</li> </ul>"},{"location":"predictors/predictor-list/#base-components","title":"Base Components","text":"<ul> <li><code>myocr.base.Predictor</code>: A simple wrapper that calls the <code>ParamConverter</code>'s input conversion, the <code>Model</code>'s forward pass, and the <code>ParamConverter</code>'s output conversion.</li> <li><code>myocr.base.ParamConverter</code>: An abstract base class defining <code>convert_input</code> and <code>convert_output</code> methods.</li> <li><code>myocr.predictors.base</code>: Defines common data structures like <code>BoundingBox</code>, <code>RectBoundingBox</code>, <code>DetectedObjects</code>, <code>TextItem</code>, and <code>RecognizedTexts</code> used as inputs and outputs by different converters.</li> </ul>"},{"location":"predictors/predictor-list/#available-predictors-and-converters","title":"Available Predictors and Converters","text":"<p>Predictors are implicitly created when calling the <code>.predictor(converter)</code> method on a loaded <code>Model</code> instance. The key components are the <code>ParamConverter</code> implementations:</p>"},{"location":"predictors/predictor-list/#1-text-detection-textdetectionparamconverter","title":"1. Text Detection (<code>TextDetectionParamConverter</code>)","text":"<ul> <li>File: <code>myocr/predictors/text_detection_predictor.py</code></li> <li>Input: <code>PIL.Image</code></li> <li>Output: <code>DetectedObjects</code> (containing original image and list of <code>RectBoundingBox</code>)</li> <li>Associated Model: Typically a DBNet/DBNet++ ONNX model.</li> <li>Preprocessing (<code>convert_input</code>):<ul> <li>Resizes image dimensions to be divisible by 32.</li> <li>Normalizes pixel values (subtract mean, divide by std).</li> <li>Transposes channels to CHW format.</li> <li>Adds batch dimension.</li> </ul> </li> <li>Postprocessing (<code>convert_output</code>):<ul> <li>Takes the raw probability map from the model.</li> <li>Applies a threshold (0.3) to create a binary map.</li> <li>Finds contours in the binary map.</li> <li>Filters contours based on length.</li> <li>Calculates minimum area rotated rectangles (<code>cv2.minAreaRect</code>).</li> <li>Filters rectangles based on minimum side length.</li> <li>Calculates a confidence score based on the mean probability within the contour.</li> <li>Filters based on confidence score (&gt;= 0.3).</li> <li>Expands the bounding box polygon (<code>_unclip</code> function with ratio 2.3).</li> <li>Calculates the minimum area rectangle of the expanded box.</li> <li>Filters again by minimum side length.</li> <li>Scales the final box coordinates back to the original image dimensions.</li> <li>Creates <code>RectBoundingBox</code> objects.</li> <li>Sorts boxes top-to-bottom, then left-to-right.</li> <li>Wraps results in a <code>DetectedObjects</code> container.</li> </ul> </li> </ul>"},{"location":"predictors/predictor-list/#2-text-direction-classification-textdirectionparamconverter","title":"2. Text Direction Classification (<code>TextDirectionParamConverter</code>)","text":"<ul> <li>File: <code>myocr/predictors/text_direction_predictor.py</code></li> <li>Input: <code>DetectedObjects</code></li> <li>Output: <code>DetectedObjects</code> (with <code>angle</code> attribute updated in each <code>RectBoundingBox</code>)</li> <li>Associated Model: Typically a simple CNN classifier ONNX model.</li> <li>Preprocessing (<code>convert_input</code>):<ul> <li>Iterates through bounding boxes in the input <code>DetectedObjects</code>.</li> <li>Crops each text region from the original image using <code>myocr.util.crop_rectangle</code> (target height 48).</li> <li>Stores the cropped image within the <code>RectBoundingBox</code> object (<code>set_croped_img</code>).</li> <li>Normalizes the cropped image pixels (<code>/ 255.0 - 0.5) / 0.5</code>).</li> <li>Ensures 3 channels (expands dims if grayscale).</li> <li>Pads batches horizontally to the maximum width found in the batch.</li> <li>Stacks images into a batch tensor (BCHW).</li> </ul> </li> <li>Postprocessing (<code>convert_output</code>):<ul> <li>Takes the raw classification logits/probabilities from the model.</li> <li>Finds the index of the maximum probability for each box (0 or 1).</li> <li>Maps the index to an angle (0 or 180 degrees).</li> <li>Calculates confidence score (probability of the predicted class).</li> <li>Updates the <code>.angle</code> attribute of the corresponding <code>RectBoundingBox</code> in the input <code>DetectedObjects</code>.</li> <li>Returns the modified <code>DetectedObjects</code>.</li> </ul> </li> </ul>"},{"location":"predictors/predictor-list/#3-text-recognition-textrecognitionparamconverter","title":"3. Text Recognition (<code>TextRecognitionParamConverter</code>)","text":"<ul> <li>File: <code>myocr/predictors/text_recognition_predictor.py</code></li> <li>Input: <code>DetectedObjects</code> (output from Text Direction)</li> <li>Output: <code>RecognizedTexts</code> (containing list of <code>TextItem</code>)</li> <li>Associated Model: Typically a CRNN-based ONNX model.</li> <li>Preprocessing (<code>convert_input</code>):<ul> <li>Retrieves the pre-cropped image stored in each <code>RectBoundingBox</code> (<code>get_croped_img</code>).</li> <li>Rotates the image if the <code>angle</code> attribute indicates 180 degrees.</li> <li>Normalizes pixel values (<code>/ 255.0 - 0.5) / 0.5</code>).</li> <li>Ensures 3 channels.</li> <li>Pads batches horizontally to the maximum width.</li> <li>Stacks images into a batch tensor (BCHW).</li> </ul> </li> <li>Postprocessing (<code>convert_output</code>):<ul> <li>Takes the raw sequence output from the model (Time Steps, Batch Size, Num Classes).</li> <li>Transposes to (Batch Size, Time Steps, Num Classes).</li> <li>Iterates through each item in the batch:<ul> <li>Applies Softmax to get probabilities.</li> <li>Calculates character confidences (max probability per time step).</li> <li>Calculates overall text confidence (mean of character confidences).</li> <li>Performs CTC decoding: Gets character indices (argmax per time step) and uses <code>myocr.util.LabelTranslator</code> (initialized with a large Chinese+English alphabet) to decode the sequence, removing blanks and duplicates.</li> <li>Creates a <code>TextItem</code> with the decoded text, confidence, and the original <code>RectBoundingBox</code>.</li> </ul> </li> <li>Collects all <code>TextItem</code>s into a <code>RecognizedTexts</code> object.</li> </ul> </li> </ul>"},{"location":"predictors/predictor-list/#usage-example-conceptual","title":"Usage Example (Conceptual)","text":"<pre><code>from myocr.modeling.model import ModelLoader, Device\nfrom myocr.predictors import TextDetectionParamConverter, TextRecognitionParamConverter, TextDirectionParamConverter\nfrom PIL import Image\n\n# Assume models are loaded\ndet_model = ModelLoader().load('onnx', 'path/to/det_model.onnx', Device('cuda:0'))\ncls_model = ModelLoader().load('onnx', 'path/to/cls_model.onnx', Device('cuda:0'))\nrec_model = ModelLoader().load('onnx', 'path/to/rec_model.onnx', Device('cuda:0'))\n\n# Create predictors by associating models with converters\ndet_predictor = det_model.predictor(TextDetectionParamConverter(det_model.device))\ncls_predictor = cls_model.predictor(TextDirectionParamConverter())\nrec_predictor = rec_model.predictor(TextRecognitionParamConverter())\n\n# Load image\nimg = Image.open('path/to/image.png').convert(\"RGB\")\n\n# Run prediction steps\ndetected_objects = det_predictor.predict(img)\nif detected_objects:\n  classified_objects = cls_predictor.predict(detected_objects) # Predict calls the converter steps + model forward\n  recognized_texts = rec_predictor.predict(classified_objects)\n\n  print(recognized_texts.get_content_text())\n</code></pre>"},{"location":"predictors/predictor-list/#predictor-configuration","title":"Predictor Configuration","text":"<p>Predictors can be configured through their parameter converters:</p> <pre><code>class CustomParamConverter(ParamConverter):\n    def convert_input(self, input_data):\n        # Custom input preprocessing\n        pass\n\n    def convert_output(self, internal_result):\n        # Custom output postprocessing\n        pass\n</code></pre>"},{"location":"predictors/predictor-list/#inputoutput-formats","title":"Input/Output Formats","text":""},{"location":"predictors/predictor-list/#text-detection","title":"Text Detection","text":"<p>Input: - RGB image (PIL.Image) - Size: Any (resized internally)</p> <p>Output: - List of text regions - Each region contains:   - Polygon points   - Confidence score   - Bounding box</p>"},{"location":"predictors/predictor-list/#text-recognition","title":"Text Recognition","text":"<p>Input: - Cropped text region (PIL.Image) - Size: 100x32 (resized internally)</p> <p>Output: - Recognized text - Confidence score - Character-level scores</p>"},{"location":"predictors/predictor-list/#text-direction","title":"Text Direction","text":"<p>Input: - Text region (PIL.Image) - Size: 32x32 (resized internally)</p> <p>Output: - Orientation angle (0\u00b0 or 180\u00b0) - Confidence score</p>"},{"location":"predictors/predictor-list/#performance-tips","title":"Performance Tips","text":""},{"location":"predictors/predictor-list/#batch-processing","title":"Batch Processing","text":"<pre><code># Process multiple regions\nresults = [predictor.predict(region) for region in regions]\n</code></pre>"},{"location":"predictors/predictor-list/#memory-optimization","title":"Memory Optimization","text":"<pre><code># Clear GPU memory\nimport torch\ntorch.cuda.empty_cache()\n</code></pre>"},{"location":"predictors/predictor-list/#input-preprocessing","title":"Input Preprocessing","text":"<pre><code># Custom preprocessing\ndef preprocess(image):\n    # Add custom preprocessing steps\n    return processed_image\n</code></pre>"},{"location":"predictors/predictor-list/#error-handling","title":"Error Handling","text":"<p>Predictors handle various error cases:</p> <ul> <li>Invalid input format</li> <li>Model loading errors</li> <li>GPU memory issues</li> <li>Inference errors</li> </ul> <p>See the Troubleshooting Guide for common issues and solutions. </p>"},{"location":"zh/","title":"myocr","text":"<p>\u200b\u4ece\u200b\u8fd9\u91cc\u200b\u5f00\u59cb\u200b</p> <p>installation overview <pre><code>```{toctree}\n:hidden:\n:caption: Development\n\nCHANGELOG\nCONTRIBUTING\nLicense &lt;https://github.com/robbyzhaox/myocr/myocr/LICENSE&gt;\nGitHub Repository &lt;https://github.com/robbyzhaox/myocr&gt;\n</code></pre></p>"},{"location":"zh/#indices-and-tables","title":"Indices and tables","text":"<pre><code>* :ref:`genindex`\n* :ref:`modindex`\n</code></pre>"},{"location":"zh/getting-started/installation/","title":"\u5b89\u88c5","text":"<p>\u200b\u5b89\u88c5\u200b</p>"},{"location":"zh/getting-started/overview/","title":"\u6982\u8ff0","text":"<p>\u200b\u6982\u89c8\u200b</p>"}]}