{"config":{"lang":["en","zh"],"separator":"[\\s\\u200b\\-_,:!=\\[\\: )\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"","text":""},{"location":"#introduction","title":"\ud83d\udd0d Introduction","text":"<p>MyOCR is a development kit for engineers to easiy train and assemble their models to predictors and pipelines for their OCR business.</p>"},{"location":"#what-can-myocr-do","title":"\ud83d\udcd6 What Can MyOCR Do?","text":"<ul> <li> <p>\ud83d\ude80 Build Your Own OCR Solutions We can quickly build our own OCR solutions like building blocks based on the components.</p> </li> <li> <p>\ud83d\ude80 Easily integrate the Model and Train With the moduler design, we can easily integrate our custom model to MyOCR to extend the components.</p> </li> <li> <p>\ud83d\ude80 Support Multiple Ways of Inference MyOCR can be used as python package to integrated to your projects, and we also support inference via Rest API.</p> </li> </ul>"},{"location":"#qucik-start","title":"\ud83d\udcdd Qucik Start","text":"<p>Tip</p> <p>MyOCR work good on CPU and GPU, we recommend to use GPU to train and inference.</p>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>Python 3.11+</li> <li>CUDA 12.6+</li> </ul>"},{"location":"#installation-method","title":"Installation Method","text":"<pre><code># Clone the code from GitHub\ngit clone https://github.com/robbyzhaox/myocr.git\ncd myocr\n\n# Install dependencies\npip install -e .\n\n# Development environment installation\npip install -e \".[dev]\"\n\n# Download pre-trained models\nmkdir -p ~/.MyOCR/models/\ncurl -fsSL \"https://drive.google.com/file/d/1b5I8Do4ODU9xE_dinDGZMraq4GDgHPH9/view?usp=drive_link\" -o ~/.MyOCR/models/dbnet++.onnx\ncurl -fsSL \"https://drive.google.com/file/d/1MSF7ArwmRjM4anDiMnqhlzj1GE_J7gnX/view?usp=drive_link\" -o ~/.MyOCR/models/rec.onnx\ncurl -fsSL \"https://drive.google.com/file/d/1TCu3vAXNVmPBY2KtoEBTGOE6tpma0puX/view?usp=drive_link\" -o ~/.MyOCR/models/cls.onnx\n</code></pre>"},{"location":"#local-inference","title":"Local Inference","text":"<p>Basic OCR Recognition</p> Common OCRStructured OCR Output (Example: Invoice Information Extraction) <pre><code>from myocr.pipelines.common_ocr_pipeline import CommonOCRPipeline\n\n# Initialize OCR pipeline (using GPU)\npipeline = CommonOCRPipeline(\"cuda:0\")  # Use \"cpu\" for CPU mode\n\n# Perform OCR recognition on an image\nresult = pipeline(\"path/to/your/image.jpg\")\nprint(result)\n</code></pre> Click to See Output Output JsonOutput Image <pre><code>[(text=\u200b\u8d38\u6613\u6218\u200b, confidence=0.0004101736412849277, bounding_box=(left=455, bottom=43, right=537, top=4, angle=(0, np.float32(0.99804187)), score=0.96864689904632))\n, (text=Text, confidence=0.0004079231293871999, bounding_box=(left=308, bottom=49, right=363, top=13, angle=(0, np.float32(0.6283184)), score=0.8928107453717127))\n, (text=\u200b\u6587\u5e93\u200b, confidence=0.0004101540253031999, bounding_box=(left=14, bottom=93, right=79, top=49, angle=(180, np.float32(0.92576)), score=0.8890029720334343))\n, (text=\u200b\u56fe\u7247\u200b, confidence=0.0004101961385458708, bounding_box=(left=216, bottom=94, right=278, top=49, angle=(180, np.float32(0.99998415)), score=0.857502628267903))\n, (text=\u200b\u5730\u56fe\u200b, confidence=0.0004101953818462789, bounding_box=(left=516, bottom=94, right=576, top=49, angle=(180, np.float32(0.9999981)), score=0.872837122885572))\n, (text=\u200b\u8d34\u200b\u5427\u200b, confidence=0.0004101790254935622, bounding_box=(left=415, bottom=94, right=480, top=50, angle=(180, np.float32(0.99998367)), score=0.8831927942269014))\n, (text=\u200b\u7f51\u76d8\u200b, confidence=0.00041003606747835875, bounding_box=(left=115, bottom=96, right=179, top=52, angle=(180, np.float32(0.815267)), score=0.9080475783482278))\n, (text=\u200b\u89c6\u9891\u200b, confidence=0.00041019057971425354, bounding_box=(left=315, bottom=96, right=379, top=52, angle=(180, np.float32(0.99999964)), score=0.9084148499800161))\n, (text=\u200b\u65b0\u95fb\u200b, confidence=0.0004098160716239363, bounding_box=(left=751, bottom=93, right=800, top=53, angle=(180, np.float32(0.9999683)), score=0.9706179747978847))\n, (text=hao123, confidence=0.0004040453059133142, bounding_box=(left=614, bottom=89, right=714, top=54, angle=(180, np.float32(1.0)), score=0.860578941761776))\n]\n</code></pre> <p><p></p></p> <pre><code>from pydantic import BaseModel, Field\nfrom myocr.pipelines.structured_output_pipeline import StructuredOutputOCRPipeline\n\n# Define output data model\nclass InvoiceItem(BaseModel):\n    name: str = Field(description=\"Item name in the invoice\")\n    price: float = Field(description=\"Item unit price\")\n    number: str = Field(description=\"Item quantity\")\n    tax: str = Field(description=\"Item tax amount\")\n\nclass InvoiceModel(BaseModel):\n    invoiceNumber: str = Field(description=\"Invoice number\")\n    invoiceDate: str = Field(description=\"Invoice date\")\n    invoiceItems: list[InvoiceItem] = Field(description=\"List of items in the invoice\")\n    totalAmount: float = Field(description=\"Total amount of the invoice\")\n\n    def to_dict(self):\n        self.__dict__[\"invoiceItems\"] = [item.__dict__ for item in self.invoiceItems]\n        return self.__dict__\n\n# Initialize structured OCR pipeline\npipeline = StructuredOutputOCRPipeline(\"cuda:0\", InvoiceModel)\n\n# Process image and get structured data\nresult = pipeline(\"path/to/invoice.jpg\")\nprint(result.to_dict())\n</code></pre> Click to See Output Output JsonOutput Image <pre><code>{\"data\":{\"invoiceDate\":\"2018\u200b\u5e74\u200b08\u200b\u6708\u200b15\u200b\u65e5\u200b\",\"invoiceItems\":[{\"name\":\"\u200b\u98de\u5229\u6d66\u200bBDL6530QT (\u200b\u667a\u80fd\u200b\u4f1a\u8bae\u200b\u7535\u5b50\u767d\u677f\u200b\u4f1a\u8bae\u200b\u5e73\u677f\u200b\u89e6\u6478\u200b\u4e00\u4f53\u200b)\",\"number\":\"One unit\",\"price\":11206.03,\"tax\":\"When it comes to the tax information, the exact rate and amount should be considered. This entry's tax is listed as 1792.97 at a 16% rate in the given invoice text.\"}],\"invoiceNumber\":\"21572\",\"totalAmount\":12999.0}}\n</code></pre> <p><p></p></p>"},{"location":"#rest-api","title":"Rest API","text":"<p>The framework provides a simple Flask API service that can be called via HTTP interface:</p> <pre><code># Start the service\npython main.py\n</code></pre> <p>API endpoints: - <code>GET /ping</code>: Check if the service is running properly - <code>POST /ocr</code>: Basic OCR recognition - <code>POST /ocr-json</code>: Structured OCR output</p> <p>We also have a UI for these endpoints, please refer to text</p>"},{"location":"#discussion","title":"\ud83d\udcac Discussion","text":"<p>We encourage to discuss and imporve this project to help more people. Github issues, discussions and the comments under this doc site are all available ways to discuss, try to use simple and clear language to ask questions, describe ideas, and seek help.</p>"},{"location":"#license","title":"\ud83d\udcc4 License","text":"<p>Apache 2.0 license</p>"},{"location":"CHANGELOG/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"CHANGELOG/#unreleased","title":"Unreleased","text":""},{"location":"CONTRIBUTING/","title":"Contributing to MyOCR","text":"<p>Thank you for your interest in contributing to MyOCR! This document provides guidelines and instructions for contributing to this project.</p>"},{"location":"CONTRIBUTING/#code-of-conduct","title":"Code of Conduct","text":"<p>By participating in this project, you agree to maintain a respectful and inclusive environment for everyone. Please be kind, considerate, and constructive in your communication.</p>"},{"location":"CONTRIBUTING/#getting-started","title":"Getting Started","text":"<ol> <li>Fork the repository: Create your own fork of the repository on GitHub.</li> <li>Clone your fork:     <pre><code>git clone https://github.com/your-username/myocr.git\ncd myocr\n</code></pre></li> <li>Add the upstream repository:    <pre><code>git remote add upstream https://github.com/robbyzhaox/myocr.git\n</code></pre></li> <li>Create a branch: Create a new branch for your work.    <pre><code>git checkout -b feature/your-feature-name\n</code></pre></li> </ol>"},{"location":"CONTRIBUTING/#development-setup","title":"Development Setup","text":"<ol> <li> <p>Install dependencies:    <pre><code>pip install -e \".[dev]\"\n</code></pre>    This installs the package in development mode with all development dependencies.</p> </li> <li> <p>Set up pre-commit hooks (optional but recommended):    <pre><code>pre-commit install\n</code></pre></p> </li> </ol>"},{"location":"CONTRIBUTING/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Keep changes focused: Each PR should address a specific feature, bug fix, or improvement.</li> <li>Update documentation: Ensure that documentation is updated to reflect your changes.</li> <li>Write tests: Add or update tests for the changes you've made.</li> <li>Run tests locally: Make sure all tests pass before submitting your PR.    <pre><code>pytest\n</code></pre></li> <li>Submit the PR: Push your changes to your fork and create a PR against the main repository.    <pre><code>git push origin feature/your-feature-name\n</code></pre></li> <li>PR Description: Provide a clear description of the changes and reference any related issues.</li> <li>Code Review: Be responsive to code review comments and make necessary adjustments.</li> </ol>"},{"location":"CONTRIBUTING/#coding-standards","title":"Coding Standards","text":"<p>We use several tools to enforce coding standards. The easiest way to ensure your code meets these standards is by using the provided Makefile commands:</p>"},{"location":"CONTRIBUTING/#using-the-makefile","title":"Using the Makefile","text":"<pre><code># Format all code (isort, black, ruff fix)\nmake run-format\n\n# Run code quality checks (isort, black, ruff, mypy, pytest)\nmake run-checks\n</code></pre>"},{"location":"CONTRIBUTING/#individual-tools","title":"Individual Tools","text":"<p>If you prefer to run the tools individually:</p> <ol> <li> <p>Black: For code formatting    <pre><code>black .\n</code></pre></p> </li> <li> <p>isort: For import sorting    <pre><code>isort .\n</code></pre></p> </li> <li> <p>Ruff: For linting    <pre><code>ruff check .\n</code></pre></p> </li> <li> <p>mypy: For type checking    <pre><code>mypy myocr\n</code></pre></p> </li> </ol> <p>The configuration for these tools is in the <code>pyproject.toml</code> file.</p>"},{"location":"CONTRIBUTING/#testing-guidelines","title":"Testing Guidelines","text":"<ol> <li>Write unit tests: Write comprehensive tests for new features and bug fixes.</li> <li>Test Coverage: Aim for high test coverage for all new code.</li> <li>Test Directory Structure: </li> <li>Place tests in the <code>tests/</code> directory</li> <li>Follow the same directory structure as the source code</li> </ol>"},{"location":"CONTRIBUTING/#documentation","title":"Documentation","text":"<p>Good documentation is crucial for the project:</p> <ol> <li>Docstrings: Add docstrings to all public classes and functions.</li> <li>Example Usage: Include example usage in docstrings where appropriate.</li> <li>README Updates: Update the README if you add major features or change functionality.</li> <li>API Documentation: For significant additions, consider updating the API documentation.</li> </ol>"},{"location":"CONTRIBUTING/#building-documentation","title":"Building Documentation","text":"<p>You can build the documentation locally using:</p> <pre><code>make docs\n</code></pre> <p>This command will generate HTML documentation and start a local server to view it.</p>"},{"location":"CONTRIBUTING/#issue-reporting","title":"Issue Reporting","text":"<p>Before creating a new issue:</p> <ol> <li>Check existing issues: Make sure the issue hasn't already been reported.</li> <li>Provide information: Include detailed information about the problem:</li> <li>Steps to reproduce</li> <li>Expected behavior</li> <li>Actual behavior</li> <li>Environment (OS, Python version, etc.)</li> <li>Logs or error messages</li> <li>Use templates: If available, use the issue templates provided in the repository.</li> </ol>"},{"location":"CONTRIBUTING/#adding-new-features","title":"Adding New Features","text":"<p>When proposing new features:</p> <ol> <li>Discuss first: For major features, open an issue to discuss the feature before implementing it.</li> <li>Modular approach: Keep the modular architecture in mind when designing new features.</li> <li>Pipeline integration: Ensure that new components integrate well with the existing pipeline structure.</li> <li>Model compatibility: If adding new models, ensure they can be loaded with the existing ModelZoo system.</li> </ol>"},{"location":"CONTRIBUTING/#docker-development","title":"Docker Development","text":"<p>We provide a utility script to simplify the Docker build and deployment process:</p>"},{"location":"CONTRIBUTING/#using-the-build-script","title":"Using the Build Script","text":"<p>The <code>scripts/build_docker_image.sh</code> script automates the process of building and running a Docker container:</p> <pre><code># Make the script executable if it's not already\nchmod +x scripts/build_docker_image.sh\n\n# Run the script\n./scripts/build_docker_image.sh\n</code></pre> <p>This script: 1. Stops and removes any existing containers based on the MyOCR image 2. Removes any existing MyOCR Docker images 3. Copies models from your local configuration 4. Builds a new Docker image using the GPU-enabled Dockerfile 5. Runs a container exposing the service on port 8000</p>"},{"location":"CONTRIBUTING/#manual-docker-build","title":"Manual Docker Build","text":"<p>If you prefer to build the Docker image manually, or need to customize the process:</p> <pre><code># For GPU version\ndocker build -f Dockerfile-infer-GPU -t myocr:custom .\n\n# For CPU version\ndocker build -f Dockerfile-infer-CPU -t myocr:custom-cpu .\n\n# Run with custom options\ndocker run -d -p 8000:8000 -v /path/to/local/models:/app/models myocr:custom\n</code></pre>"},{"location":"CONTRIBUTING/#license","title":"License","text":"<p>By contributing to MyOCR, you agree that your contributions will be licensed under the project's Apache 2.0 license.</p> <p>Thank you for contributing to MyOCR! Your efforts help make this project better for everyone. </p>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.11+</li> <li>CUDA 12.6+ (Recommended for GPU acceleration, but CPU mode is also supported)</li> </ul>"},{"location":"getting-started/installation/#installation-method","title":"Installation Method","text":"<pre><code># Clone the code from GitHub\ngit clone https://github.com/robbyzhaox/myocr.git\ncd myocr\n\n# Install dependencies\npip install -e .\n\n# Development environment installation\npip install -e \".[dev]\"\n\n# Download pre-trained models\nmkdir -p ~/.MyOCR/models/\ncurl -fsSL \"https://drive.google.com/file/d/1b5I8Do4ODU9xE_dinDGZMraq4GDgHPH9/view?usp=drive_link\" -o ~/.MyOCR/models/dbnet++.onnx\ncurl -fsSL \"https://drive.google.com/file/d/1MSF7ArwmRjM4anDiMnqhlzj1GE_J7gnX/view?usp=drive_link\" -o ~/.MyOCR/models/rec.onnx\ncurl -fsSL \"https://drive.google.com/file/d/1TCu3vAXNVmPBY2KtoEBTGOE6tpma0puX/view?usp=drive_link\" -o ~/.MyOCR/models/cls.onnx\n</code></pre>"},{"location":"getting-started/installation/#quick-start","title":"Quick Start","text":""},{"location":"getting-started/installation/#basic-ocr-recognition","title":"Basic OCR Recognition","text":"<pre><code>from myocr.pipelines.common_ocr_pipeline import CommonOCRPipeline\n\n# Initialize OCR pipeline (using GPU)\npipeline = CommonOCRPipeline(\"cuda:0\")  # Use \"cpu\" for CPU mode\n\n# Perform OCR recognition on an image\nresult = pipeline(\"path/to/your/image.jpg\")\nprint(result)\n</code></pre>"},{"location":"getting-started/installation/#structured-ocr-output-example-invoice-information-extraction","title":"Structured OCR Output (Example: Invoice Information Extraction)","text":"<pre><code>from pydantic import BaseModel, Field\nfrom myocr.pipelines.structured_output_pipeline import StructuredOutputOCRPipeline\n\n# Define output data model\nclass InvoiceItem(BaseModel):\n    name: str = Field(description=\"Item name in the invoice\")\n    price: float = Field(description=\"Item unit price\")\n    number: str = Field(description=\"Item quantity\")\n    tax: str = Field(description=\"Item tax amount\")\n\nclass InvoiceModel(BaseModel):\n    invoiceNumber: str = Field(description=\"Invoice number\")\n    invoiceDate: str = Field(description=\"Invoice date\")\n    invoiceItems: list[InvoiceItem] = Field(description=\"List of items in the invoice\")\n    totalAmount: float = Field(description=\"Total amount of the invoice\")\n\n    def to_dict(self):\n        self.__dict__[\"invoiceItems\"] = [item.__dict__ for item in self.invoiceItems]\n        return self.__dict__\n\n# Initialize structured OCR pipeline\npipeline = StructuredOutputOCRPipeline(\"cuda:0\", InvoiceModel)\n\n# Process image and get structured data\nresult = pipeline(\"path/to/invoice.jpg\")\nprint(result.to_dict())\n</code></pre>"},{"location":"getting-started/installation/#using-http-api-service","title":"Using HTTP API Service","text":"<p>The framework provides a simple Flask API service that can be called via HTTP interface:</p> <pre><code># Start the service\npython main.py\n</code></pre> <p>API endpoints: - <code>GET /ping</code>: Check if the service is running properly - <code>POST /ocr</code>: Basic OCR recognition - <code>POST /ocr-json</code>: Structured OCR output</p> <p>We also have a UI for these endpoints, please refer to text</p>"},{"location":"getting-started/overview/","title":"Overview","text":""},{"location":"getting-started/overview/#myocr-components","title":"MyOCR Components","text":"<p>Model is artificial neural network defined by Pytorch, it consists two parts, the model architecture and model weights, a model architecture usually build with transform, backbone, neck and head. MyOCR also have a tool for easily train the model by custom data.</p> <p>The Model Class has serval sub classes: PytorchModel, OnnxMode, CustomModel.</p> <p>Converter is a component for building Predictor, a Converter is responsible for preparing appropriate parameters for a model, and then convert the model output to a specific type.</p> <p>Predictor is built on Model and Converter, it is a component to do a real world task by the neural network. For example, we have a MLP model for classifing images, usually the model accepts a tensor represents the image and output a vector representing the probability of each class, but for a user we just want give an image and got the class name with a confidence, thus a converter will help to eliminate the gap of the input and output we want and network provides. By using a Model and a Predictor we will got a Predictor for us to do a real world task, for the above image classification task, the input for Predictor can be an image and output can be a class name.</p> <p>Pipeline is the arrangement and combination of different Predictors. It is used to solve a more complex problem, which may have multiple steps and require a combination of multiple models to complete. Such as for a traditional image OCR task, we at least need a detection model and a recognition model, detection model to find where there is text in the image, and recognition model to extract specific text content.</p>"},{"location":"getting-started/overview/#customization-and-extension","title":"Customization and Extension","text":""},{"location":"getting-started/overview/#adding-new-structured-output-models","title":"Adding New Structured Output Models","text":"<ol> <li>Define your data model using Pydantic:</li> </ol> <pre><code>from pydantic import BaseModel, Field\n\nclass CustomModel(BaseModel):\n    field1: str = Field(description=\"Description of field1\")\n    field2: int = Field(description=\"Description of field2\")\n    # Add more fields...\n</code></pre> <ol> <li>Create a new pipeline with your model:</li> </ol> <pre><code>from myocr.pipelines.structured_output_pipeline import StructuredOutputOCRPipeline\n\npipeline = StructuredOutputOCRPipeline(\"cuda:0\", CustomModel)\n</code></pre>"},{"location":"getting-started/overview/#replacing-or-adding-new-models","title":"Replacing or Adding New Models","text":"<ol> <li>Place ONNX format model files in the <code>myocr/models</code> directory</li> <li>Modify the configuration file to use the new models:</li> </ol> <pre><code>model:\n  detection: \"path/to/your/detection_model.onnx\"\n  recognition: \"path/to/your/recognition_model.onnx\"\n</code></pre>"},{"location":"inference/local/","title":"Local Inference","text":"<p>local</p>"},{"location":"inference/rest/","title":"Restful API","text":"<p>rest api</p>"},{"location":"models/model-list/","title":"Model List","text":"<p>All the models</p>"},{"location":"pipelines/ocr/","title":"Common OCR","text":""},{"location":"pipelines/ocr/#ocr","title":"OCR","text":""},{"location":"pipelines/structured-ocr/","title":"Structured Extractor","text":"<p>structured-ocr.md</p>"},{"location":"predictors/converters/","title":"Create Converters","text":""},{"location":"predictors/converters/#converters","title":"Converters","text":""},{"location":"predictors/predictors/","title":"Build Predictors","text":""},{"location":"predictors/predictors/#predictors","title":"Predictors","text":""},{"location":"zh/","title":"myocr","text":"<p>\u200b\u4ece\u200b\u8fd9\u91cc\u200b\u5f00\u59cb\u200b</p> <p>installation overview <pre><code>```{toctree}\n:hidden:\n:caption: Development\n\nCHANGELOG\nCONTRIBUTING\nLicense &lt;https://github.com/robbyzhaox/myocr/myocr/LICENSE&gt;\nGitHub Repository &lt;https://github.com/robbyzhaox/myocr&gt;\n</code></pre></p>"},{"location":"zh/#indices-and-tables","title":"Indices and tables","text":"<pre><code>* :ref:`genindex`\n* :ref:`modindex`\n</code></pre>"},{"location":"zh/getting-started/installation/","title":"\u5b89\u88c5","text":"<p>\u200b\u5b89\u88c5\u200b</p>"},{"location":"zh/getting-started/overview/","title":"\u6982\u8ff0","text":"<p>\u200b\u6982\u89c8\u200b</p>"},{"location":"zh/inference/local/","title":"\u672c\u5730\u200b\u63a8\u7406","text":"<p>local</p>"},{"location":"zh/inference/rest/","title":"\u5728\u7ebf\u200bAPI","text":"<p>rest api</p>"},{"location":"zh/models/model-list/","title":"\u6a21\u578b\u200b\u5217\u8868","text":"<p>All the models</p>"},{"location":"zh/pipelines/ocr/","title":"\u901a\u7528\u200bOCR","text":""},{"location":"zh/pipelines/structured-ocr/","title":"\u63d0\u53d6\u200b\u7ed3\u6784\u5316\u200b\u4fe1\u606f","text":"<p>structured-ocr.md</p>"},{"location":"zh/predictors/converters/","title":"\u521b\u5efa\u200b\u8f6c\u6362\u5668","text":""},{"location":"zh/predictors/predictors/","title":"\u6784\u5efa\u200b\u9884\u6d4b\u5668","text":""}]}